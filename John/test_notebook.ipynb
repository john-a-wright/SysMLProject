{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\John\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from sit1m_data_preprocessing import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Sift1M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...: 0 url [00:00, ? url/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:01<00:00,  1.83s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:01<00:00,  1.83s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:01<00:00,  1.83s/ url]\n",
      "Extraction completed...: 0 file [00:01, ? file/s]\n",
      "Dl Size...: 100%|██████████| 525128288/525128288 [00:01<00:00, 285759764.45 MiB/s]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:01<00:00,  1.84s/ url]\n"
     ]
    }
   ],
   "source": [
    "# download (if not downloaded) and build sift1m dataset\n",
    "dataset_download_path = \"D:/College/SysML/dataset\" # a folder to cache dataset contents that are downloaded\n",
    "splits = build_sift1m(dataset_download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building train split array\n",
      "Found existing train data file\n",
      "[[  0.  16.  35. ...  25.  23.   1.]\n",
      " [ 14.  35.  19. ...  11.  21.  33.]\n",
      " [  0.   1.   5. ...   4.  23.  10.]\n",
      " ...\n",
      " [ 30.  12.  12. ...  50.  10.   0.]\n",
      " [  0.   5.  12. ...   1.   2.  13.]\n",
      " [114.  31.   0. ...  25.  16.   0.]]\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "# make and print train split input array (1 million embedding arrays of length 128)\n",
    "train_input_array = get_train_split(splits)\n",
    "\n",
    "print(train_input_array)\n",
    "print(train_input_array.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building test split array\n",
      "Progress: 50.0%\n",
      "Progress: 100.0%\n",
      "[[  1.   3.  11. ...  42.  48.  11.]\n",
      " [ 40.  25.  11. ...   3.  19.  13.]\n",
      " [ 28.   4.   3. ...   2.  54.  47.]\n",
      " ...\n",
      " [  0.  15.  64. ...   3.  62. 118.]\n",
      " [131.   2.   0. ...   7.   0.   0.]\n",
      " [ 23.   0.   0. ...  79.  16.   4.]]\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "# make and print test split input array (10k embedding arrays of length 128)\n",
    "test_input_array, neighbors = get_test_split(splits)\n",
    "\n",
    "print(test_input_array)\n",
    "print(test_input_array.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PQ Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = train_input_array.shape[1]\n",
    "numSubVectors = 8\n",
    "subVectorBits = 8\n",
    "\n",
    "pq = faiss.IndexPQ(dim, numSubVectors, subVectorBits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq.train(train_input_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq.add(train_input_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "distances, indexes = pq.search(test_input_array[:10000], k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5851\n"
     ]
    }
   ],
   "source": [
    "t = 25\n",
    "i = 0\n",
    "relevantPercent = []\n",
    "\n",
    "for entry in neighbors:\n",
    "\n",
    "    topIndexs = []\n",
    "    for x in range(t):\n",
    "        index_dict = entry[x]\n",
    "        topIndexs.append(index_dict[\"index\"])\n",
    "\n",
    "    search_results = indexes[i]\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for x in search_results:\n",
    "        if x in topIndexs:\n",
    "            count+=1\n",
    "\n",
    "    relevantPercent.append(count/k)\n",
    "    \n",
    "    i=i+1\n",
    "        \n",
    "\n",
    "print(sum(relevantPercent) / len(relevantPercent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-FSQ VQVAE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, input_dim, codebook_size, codebook_dim):\n",
    "        super(VQVAE, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.codebook_size = codebook_size\n",
    "        self.codebook_dim = codebook_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, codebook_dim)\n",
    "        )\n",
    "        \n",
    "        # Codebook\n",
    "        self.codebook = nn.Embedding(codebook_size, codebook_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(codebook_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, input_dim)\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return z\n",
    "\n",
    "    def quantize(self, z):\n",
    "        # Find nearest embedding in the codebook\n",
    "        z = z.unsqueeze(2)\n",
    "        distances = torch.norm(z - self.codebook.weight.unsqueeze(0), dim=1)\n",
    "        indices = torch.argmin(distances, dim=1)\n",
    "        quantized = self.codebook(indices)\n",
    "        return quantized, indices\n",
    "\n",
    "    def decode(self, z):\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        quantized, indices = self.quantize(z)\n",
    "        x_recon = self.decode(quantized)\n",
    "        return x_recon, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqvae = VQVAE(input_dim=128, codebook_size=512, codebook_dim=64)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(vqvae.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        # You may need to preprocess the data here, such as converting to tensor\n",
    "        return torch.tensor(sample, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_input_array)\n",
    "print(len(train_dataset))\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\John\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64, 1, 128])) that is different to the input size (torch.Size([64, 64, 128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Loss: 2021.46142578125\n",
      "Epoch: 0, Batch: 1, Loss: 1326.0146484375\n",
      "Epoch: 0, Batch: 2, Loss: 75255536.0\n",
      "Epoch: 0, Batch: 3, Loss: 2.6874266831991206e+30\n",
      "Epoch: 0, Batch: 4, Loss: inf\n",
      "Epoch: 0, Batch: 5, Loss: nan\n",
      "Epoch: 0, Batch: 6, Loss: nan\n",
      "Epoch: 0, Batch: 7, Loss: nan\n",
      "Epoch: 0, Batch: 8, Loss: nan\n",
      "Epoch: 0, Batch: 9, Loss: nan\n",
      "Epoch: 0, Batch: 10, Loss: nan\n",
      "Epoch: 0, Batch: 11, Loss: nan\n",
      "Epoch: 0, Batch: 12, Loss: nan\n",
      "Epoch: 0, Batch: 13, Loss: nan\n",
      "Epoch: 0, Batch: 14, Loss: nan\n",
      "Epoch: 0, Batch: 15, Loss: nan\n",
      "Epoch: 0, Batch: 16, Loss: nan\n",
      "Epoch: 0, Batch: 17, Loss: nan\n",
      "Epoch: 0, Batch: 18, Loss: nan\n",
      "Epoch: 0, Batch: 19, Loss: nan\n",
      "Epoch: 0, Batch: 20, Loss: nan\n",
      "Epoch: 0, Batch: 21, Loss: nan\n",
      "Epoch: 0, Batch: 22, Loss: nan\n",
      "Epoch: 0, Batch: 23, Loss: nan\n",
      "Epoch: 0, Batch: 24, Loss: nan\n",
      "Epoch: 0, Batch: 25, Loss: nan\n",
      "Epoch: 0, Batch: 26, Loss: nan\n",
      "Epoch: 0, Batch: 27, Loss: nan\n",
      "Epoch: 0, Batch: 28, Loss: nan\n",
      "Epoch: 0, Batch: 29, Loss: nan\n",
      "Epoch: 0, Batch: 30, Loss: nan\n",
      "Epoch: 0, Batch: 31, Loss: nan\n",
      "Epoch: 0, Batch: 32, Loss: nan\n",
      "Epoch: 0, Batch: 33, Loss: nan\n",
      "Epoch: 0, Batch: 34, Loss: nan\n",
      "Epoch: 0, Batch: 35, Loss: nan\n",
      "Epoch: 0, Batch: 36, Loss: nan\n",
      "Epoch: 0, Batch: 37, Loss: nan\n",
      "Epoch: 0, Batch: 38, Loss: nan\n",
      "Epoch: 0, Batch: 39, Loss: nan\n",
      "Epoch: 0, Batch: 40, Loss: nan\n",
      "Epoch: 0, Batch: 41, Loss: nan\n",
      "Epoch: 0, Batch: 42, Loss: nan\n",
      "Epoch: 0, Batch: 43, Loss: nan\n",
      "Epoch: 0, Batch: 44, Loss: nan\n",
      "Epoch: 0, Batch: 45, Loss: nan\n",
      "Epoch: 0, Batch: 46, Loss: nan\n",
      "Epoch: 0, Batch: 47, Loss: nan\n",
      "Epoch: 0, Batch: 48, Loss: nan\n",
      "Epoch: 0, Batch: 49, Loss: nan\n",
      "Epoch: 0, Batch: 50, Loss: nan\n",
      "Epoch: 0, Batch: 51, Loss: nan\n",
      "Epoch: 0, Batch: 52, Loss: nan\n",
      "Epoch: 0, Batch: 53, Loss: nan\n",
      "Epoch: 0, Batch: 54, Loss: nan\n",
      "Epoch: 0, Batch: 55, Loss: nan\n",
      "Epoch: 0, Batch: 56, Loss: nan\n",
      "Epoch: 0, Batch: 57, Loss: nan\n",
      "Epoch: 0, Batch: 58, Loss: nan\n",
      "Epoch: 0, Batch: 59, Loss: nan\n",
      "Epoch: 0, Batch: 60, Loss: nan\n",
      "Epoch: 0, Batch: 61, Loss: nan\n",
      "Epoch: 0, Batch: 62, Loss: nan\n",
      "Epoch: 0, Batch: 63, Loss: nan\n",
      "Epoch: 0, Batch: 64, Loss: nan\n",
      "Epoch: 0, Batch: 65, Loss: nan\n",
      "Epoch: 0, Batch: 66, Loss: nan\n",
      "Epoch: 0, Batch: 67, Loss: nan\n",
      "Epoch: 0, Batch: 68, Loss: nan\n",
      "Epoch: 0, Batch: 69, Loss: nan\n",
      "Epoch: 0, Batch: 70, Loss: nan\n",
      "Epoch: 0, Batch: 71, Loss: nan\n",
      "Epoch: 0, Batch: 72, Loss: nan\n",
      "Epoch: 0, Batch: 73, Loss: nan\n",
      "Epoch: 0, Batch: 74, Loss: nan\n",
      "Epoch: 0, Batch: 75, Loss: nan\n",
      "Epoch: 0, Batch: 76, Loss: nan\n",
      "Epoch: 0, Batch: 77, Loss: nan\n",
      "Epoch: 0, Batch: 78, Loss: nan\n",
      "Epoch: 0, Batch: 79, Loss: nan\n",
      "Epoch: 0, Batch: 80, Loss: nan\n",
      "Epoch: 0, Batch: 81, Loss: nan\n",
      "Epoch: 0, Batch: 82, Loss: nan\n",
      "Epoch: 0, Batch: 83, Loss: nan\n",
      "Epoch: 0, Batch: 84, Loss: nan\n",
      "Epoch: 0, Batch: 85, Loss: nan\n",
      "Epoch: 0, Batch: 86, Loss: nan\n",
      "Epoch: 0, Batch: 87, Loss: nan\n",
      "Epoch: 0, Batch: 88, Loss: nan\n",
      "Epoch: 0, Batch: 89, Loss: nan\n",
      "Epoch: 0, Batch: 90, Loss: nan\n",
      "Epoch: 0, Batch: 91, Loss: nan\n",
      "Epoch: 0, Batch: 92, Loss: nan\n",
      "Epoch: 0, Batch: 93, Loss: nan\n",
      "Epoch: 0, Batch: 94, Loss: nan\n",
      "Epoch: 0, Batch: 95, Loss: nan\n",
      "Epoch: 0, Batch: 96, Loss: nan\n",
      "Epoch: 0, Batch: 97, Loss: nan\n",
      "Epoch: 0, Batch: 98, Loss: nan\n",
      "Epoch: 0, Batch: 99, Loss: nan\n",
      "Epoch: 0, Batch: 100, Loss: nan\n",
      "Epoch: 0, Batch: 101, Loss: nan\n",
      "Epoch: 0, Batch: 102, Loss: nan\n",
      "Epoch: 0, Batch: 103, Loss: nan\n",
      "Epoch: 0, Batch: 104, Loss: nan\n",
      "Epoch: 0, Batch: 105, Loss: nan\n",
      "Epoch: 0, Batch: 106, Loss: nan\n",
      "Epoch: 0, Batch: 107, Loss: nan\n",
      "Epoch: 0, Batch: 108, Loss: nan\n",
      "Epoch: 0, Batch: 109, Loss: nan\n",
      "Epoch: 0, Batch: 110, Loss: nan\n",
      "Epoch: 0, Batch: 111, Loss: nan\n",
      "Epoch: 0, Batch: 112, Loss: nan\n",
      "Epoch: 0, Batch: 113, Loss: nan\n",
      "Epoch: 0, Batch: 114, Loss: nan\n",
      "Epoch: 0, Batch: 115, Loss: nan\n",
      "Epoch: 0, Batch: 116, Loss: nan\n",
      "Epoch: 0, Batch: 117, Loss: nan\n",
      "Epoch: 0, Batch: 118, Loss: nan\n",
      "Epoch: 0, Batch: 119, Loss: nan\n",
      "Epoch: 0, Batch: 120, Loss: nan\n",
      "Epoch: 0, Batch: 121, Loss: nan\n",
      "Epoch: 0, Batch: 122, Loss: nan\n",
      "Epoch: 0, Batch: 123, Loss: nan\n",
      "Epoch: 0, Batch: 124, Loss: nan\n",
      "Epoch: 0, Batch: 125, Loss: nan\n",
      "Epoch: 0, Batch: 126, Loss: nan\n",
      "Epoch: 0, Batch: 127, Loss: nan\n",
      "Epoch: 0, Batch: 128, Loss: nan\n",
      "Epoch: 0, Batch: 129, Loss: nan\n",
      "Epoch: 0, Batch: 130, Loss: nan\n",
      "Epoch: 0, Batch: 131, Loss: nan\n",
      "Epoch: 0, Batch: 132, Loss: nan\n",
      "Epoch: 0, Batch: 133, Loss: nan\n",
      "Epoch: 0, Batch: 134, Loss: nan\n",
      "Epoch: 0, Batch: 135, Loss: nan\n",
      "Epoch: 0, Batch: 136, Loss: nan\n",
      "Epoch: 0, Batch: 137, Loss: nan\n",
      "Epoch: 0, Batch: 138, Loss: nan\n",
      "Epoch: 0, Batch: 139, Loss: nan\n",
      "Epoch: 0, Batch: 140, Loss: nan\n",
      "Epoch: 0, Batch: 141, Loss: nan\n",
      "Epoch: 0, Batch: 142, Loss: nan\n",
      "Epoch: 0, Batch: 143, Loss: nan\n",
      "Epoch: 0, Batch: 144, Loss: nan\n",
      "Epoch: 0, Batch: 145, Loss: nan\n",
      "Epoch: 0, Batch: 146, Loss: nan\n",
      "Epoch: 0, Batch: 147, Loss: nan\n",
      "Epoch: 0, Batch: 148, Loss: nan\n",
      "Epoch: 0, Batch: 149, Loss: nan\n",
      "Epoch: 0, Batch: 150, Loss: nan\n",
      "Epoch: 0, Batch: 151, Loss: nan\n",
      "Epoch: 0, Batch: 152, Loss: nan\n",
      "Epoch: 0, Batch: 153, Loss: nan\n",
      "Epoch: 0, Batch: 154, Loss: nan\n",
      "Epoch: 0, Batch: 155, Loss: nan\n",
      "Epoch: 0, Batch: 156, Loss: nan\n",
      "Epoch: 0, Batch: 157, Loss: nan\n",
      "Epoch: 0, Batch: 158, Loss: nan\n",
      "Epoch: 0, Batch: 159, Loss: nan\n",
      "Epoch: 0, Batch: 160, Loss: nan\n",
      "Epoch: 0, Batch: 161, Loss: nan\n",
      "Epoch: 0, Batch: 162, Loss: nan\n",
      "Epoch: 0, Batch: 163, Loss: nan\n",
      "Epoch: 0, Batch: 164, Loss: nan\n",
      "Epoch: 0, Batch: 165, Loss: nan\n",
      "Epoch: 0, Batch: 166, Loss: nan\n",
      "Epoch: 0, Batch: 167, Loss: nan\n",
      "Epoch: 0, Batch: 168, Loss: nan\n",
      "Epoch: 0, Batch: 169, Loss: nan\n",
      "Epoch: 0, Batch: 170, Loss: nan\n",
      "Epoch: 0, Batch: 171, Loss: nan\n",
      "Epoch: 0, Batch: 172, Loss: nan\n",
      "Epoch: 0, Batch: 173, Loss: nan\n",
      "Epoch: 0, Batch: 174, Loss: nan\n",
      "Epoch: 0, Batch: 175, Loss: nan\n",
      "Epoch: 0, Batch: 176, Loss: nan\n",
      "Epoch: 0, Batch: 177, Loss: nan\n",
      "Epoch: 0, Batch: 178, Loss: nan\n",
      "Epoch: 0, Batch: 179, Loss: nan\n",
      "Epoch: 0, Batch: 180, Loss: nan\n",
      "Epoch: 0, Batch: 181, Loss: nan\n",
      "Epoch: 0, Batch: 182, Loss: nan\n",
      "Epoch: 0, Batch: 183, Loss: nan\n",
      "Epoch: 0, Batch: 184, Loss: nan\n",
      "Epoch: 0, Batch: 185, Loss: nan\n",
      "Epoch: 0, Batch: 186, Loss: nan\n",
      "Epoch: 0, Batch: 187, Loss: nan\n",
      "Epoch: 0, Batch: 188, Loss: nan\n",
      "Epoch: 0, Batch: 189, Loss: nan\n",
      "Epoch: 0, Batch: 190, Loss: nan\n",
      "Epoch: 0, Batch: 191, Loss: nan\n",
      "Epoch: 0, Batch: 192, Loss: nan\n",
      "Epoch: 0, Batch: 193, Loss: nan\n",
      "Epoch: 0, Batch: 194, Loss: nan\n",
      "Epoch: 0, Batch: 195, Loss: nan\n",
      "Epoch: 0, Batch: 196, Loss: nan\n",
      "Epoch: 0, Batch: 197, Loss: nan\n",
      "Epoch: 0, Batch: 198, Loss: nan\n",
      "Epoch: 0, Batch: 199, Loss: nan\n",
      "Epoch: 0, Batch: 200, Loss: nan\n",
      "Epoch: 0, Batch: 201, Loss: nan\n",
      "Epoch: 0, Batch: 202, Loss: nan\n",
      "Epoch: 0, Batch: 203, Loss: nan\n",
      "Epoch: 0, Batch: 204, Loss: nan\n",
      "Epoch: 0, Batch: 205, Loss: nan\n",
      "Epoch: 0, Batch: 206, Loss: nan\n",
      "Epoch: 0, Batch: 207, Loss: nan\n",
      "Epoch: 0, Batch: 208, Loss: nan\n",
      "Epoch: 0, Batch: 209, Loss: nan\n",
      "Epoch: 0, Batch: 210, Loss: nan\n",
      "Epoch: 0, Batch: 211, Loss: nan\n",
      "Epoch: 0, Batch: 212, Loss: nan\n",
      "Epoch: 0, Batch: 213, Loss: nan\n",
      "Epoch: 0, Batch: 214, Loss: nan\n",
      "Epoch: 0, Batch: 215, Loss: nan\n",
      "Epoch: 0, Batch: 216, Loss: nan\n",
      "Epoch: 0, Batch: 217, Loss: nan\n",
      "Epoch: 0, Batch: 218, Loss: nan\n",
      "Epoch: 0, Batch: 219, Loss: nan\n",
      "Epoch: 0, Batch: 220, Loss: nan\n",
      "Epoch: 0, Batch: 221, Loss: nan\n",
      "Epoch: 0, Batch: 222, Loss: nan\n",
      "Epoch: 0, Batch: 223, Loss: nan\n",
      "Epoch: 0, Batch: 224, Loss: nan\n",
      "Epoch: 0, Batch: 225, Loss: nan\n",
      "Epoch: 0, Batch: 226, Loss: nan\n",
      "Epoch: 0, Batch: 227, Loss: nan\n",
      "Epoch: 0, Batch: 228, Loss: nan\n",
      "Epoch: 0, Batch: 229, Loss: nan\n",
      "Epoch: 0, Batch: 230, Loss: nan\n",
      "Epoch: 0, Batch: 231, Loss: nan\n",
      "Epoch: 0, Batch: 232, Loss: nan\n",
      "Epoch: 0, Batch: 233, Loss: nan\n",
      "Epoch: 0, Batch: 234, Loss: nan\n",
      "Epoch: 0, Batch: 235, Loss: nan\n",
      "Epoch: 0, Batch: 236, Loss: nan\n",
      "Epoch: 0, Batch: 237, Loss: nan\n",
      "Epoch: 0, Batch: 238, Loss: nan\n",
      "Epoch: 0, Batch: 239, Loss: nan\n",
      "Epoch: 0, Batch: 240, Loss: nan\n",
      "Epoch: 0, Batch: 241, Loss: nan\n",
      "Epoch: 0, Batch: 242, Loss: nan\n",
      "Epoch: 0, Batch: 243, Loss: nan\n",
      "Epoch: 0, Batch: 244, Loss: nan\n",
      "Epoch: 0, Batch: 245, Loss: nan\n",
      "Epoch: 0, Batch: 246, Loss: nan\n",
      "Epoch: 0, Batch: 247, Loss: nan\n",
      "Epoch: 0, Batch: 248, Loss: nan\n",
      "Epoch: 0, Batch: 249, Loss: nan\n",
      "Epoch: 0, Batch: 250, Loss: nan\n",
      "Epoch: 0, Batch: 251, Loss: nan\n",
      "Epoch: 0, Batch: 252, Loss: nan\n",
      "Epoch: 0, Batch: 253, Loss: nan\n",
      "Epoch: 0, Batch: 254, Loss: nan\n",
      "Epoch: 0, Batch: 255, Loss: nan\n",
      "Epoch: 0, Batch: 256, Loss: nan\n",
      "Epoch: 0, Batch: 257, Loss: nan\n",
      "Epoch: 0, Batch: 258, Loss: nan\n",
      "Epoch: 0, Batch: 259, Loss: nan\n",
      "Epoch: 0, Batch: 260, Loss: nan\n",
      "Epoch: 0, Batch: 261, Loss: nan\n",
      "Epoch: 0, Batch: 262, Loss: nan\n",
      "Epoch: 0, Batch: 263, Loss: nan\n",
      "Epoch: 0, Batch: 264, Loss: nan\n",
      "Epoch: 0, Batch: 265, Loss: nan\n",
      "Epoch: 0, Batch: 266, Loss: nan\n",
      "Epoch: 0, Batch: 267, Loss: nan\n",
      "Epoch: 0, Batch: 268, Loss: nan\n",
      "Epoch: 0, Batch: 269, Loss: nan\n",
      "Epoch: 0, Batch: 270, Loss: nan\n",
      "Epoch: 0, Batch: 271, Loss: nan\n",
      "Epoch: 0, Batch: 272, Loss: nan\n",
      "Epoch: 0, Batch: 273, Loss: nan\n",
      "Epoch: 0, Batch: 274, Loss: nan\n",
      "Epoch: 0, Batch: 275, Loss: nan\n",
      "Epoch: 0, Batch: 276, Loss: nan\n",
      "Epoch: 0, Batch: 277, Loss: nan\n",
      "Epoch: 0, Batch: 278, Loss: nan\n",
      "Epoch: 0, Batch: 279, Loss: nan\n",
      "Epoch: 0, Batch: 280, Loss: nan\n",
      "Epoch: 0, Batch: 281, Loss: nan\n",
      "Epoch: 0, Batch: 282, Loss: nan\n",
      "Epoch: 0, Batch: 283, Loss: nan\n",
      "Epoch: 0, Batch: 284, Loss: nan\n",
      "Epoch: 0, Batch: 285, Loss: nan\n",
      "Epoch: 0, Batch: 286, Loss: nan\n",
      "Epoch: 0, Batch: 287, Loss: nan\n",
      "Epoch: 0, Batch: 288, Loss: nan\n",
      "Epoch: 0, Batch: 289, Loss: nan\n",
      "Epoch: 0, Batch: 290, Loss: nan\n",
      "Epoch: 0, Batch: 291, Loss: nan\n",
      "Epoch: 0, Batch: 292, Loss: nan\n",
      "Epoch: 0, Batch: 293, Loss: nan\n",
      "Epoch: 0, Batch: 294, Loss: nan\n",
      "Epoch: 0, Batch: 295, Loss: nan\n",
      "Epoch: 0, Batch: 296, Loss: nan\n",
      "Epoch: 0, Batch: 297, Loss: nan\n",
      "Epoch: 0, Batch: 298, Loss: nan\n",
      "Epoch: 0, Batch: 299, Loss: nan\n",
      "Epoch: 0, Batch: 300, Loss: nan\n",
      "Epoch: 0, Batch: 301, Loss: nan\n",
      "Epoch: 0, Batch: 302, Loss: nan\n",
      "Epoch: 0, Batch: 303, Loss: nan\n",
      "Epoch: 0, Batch: 304, Loss: nan\n",
      "Epoch: 0, Batch: 305, Loss: nan\n",
      "Epoch: 0, Batch: 306, Loss: nan\n",
      "Epoch: 0, Batch: 307, Loss: nan\n",
      "Epoch: 0, Batch: 308, Loss: nan\n",
      "Epoch: 0, Batch: 309, Loss: nan\n",
      "Epoch: 0, Batch: 310, Loss: nan\n",
      "Epoch: 0, Batch: 311, Loss: nan\n",
      "Epoch: 0, Batch: 312, Loss: nan\n",
      "Epoch: 0, Batch: 313, Loss: nan\n",
      "Epoch: 0, Batch: 314, Loss: nan\n",
      "Epoch: 0, Batch: 315, Loss: nan\n",
      "Epoch: 0, Batch: 316, Loss: nan\n",
      "Epoch: 0, Batch: 317, Loss: nan\n",
      "Epoch: 0, Batch: 318, Loss: nan\n",
      "Epoch: 0, Batch: 319, Loss: nan\n",
      "Epoch: 0, Batch: 320, Loss: nan\n",
      "Epoch: 0, Batch: 321, Loss: nan\n",
      "Epoch: 0, Batch: 322, Loss: nan\n",
      "Epoch: 0, Batch: 323, Loss: nan\n",
      "Epoch: 0, Batch: 324, Loss: nan\n",
      "Epoch: 0, Batch: 325, Loss: nan\n",
      "Epoch: 0, Batch: 326, Loss: nan\n",
      "Epoch: 0, Batch: 327, Loss: nan\n",
      "Epoch: 0, Batch: 328, Loss: nan\n",
      "Epoch: 0, Batch: 329, Loss: nan\n",
      "Epoch: 0, Batch: 330, Loss: nan\n",
      "Epoch: 0, Batch: 331, Loss: nan\n",
      "Epoch: 0, Batch: 332, Loss: nan\n",
      "Epoch: 0, Batch: 333, Loss: nan\n",
      "Epoch: 0, Batch: 334, Loss: nan\n",
      "Epoch: 0, Batch: 335, Loss: nan\n",
      "Epoch: 0, Batch: 336, Loss: nan\n",
      "Epoch: 0, Batch: 337, Loss: nan\n",
      "Epoch: 0, Batch: 338, Loss: nan\n",
      "Epoch: 0, Batch: 339, Loss: nan\n",
      "Epoch: 0, Batch: 340, Loss: nan\n",
      "Epoch: 0, Batch: 341, Loss: nan\n",
      "Epoch: 0, Batch: 342, Loss: nan\n",
      "Epoch: 0, Batch: 343, Loss: nan\n",
      "Epoch: 0, Batch: 344, Loss: nan\n",
      "Epoch: 0, Batch: 345, Loss: nan\n",
      "Epoch: 0, Batch: 346, Loss: nan\n",
      "Epoch: 0, Batch: 347, Loss: nan\n",
      "Epoch: 0, Batch: 348, Loss: nan\n",
      "Epoch: 0, Batch: 349, Loss: nan\n",
      "Epoch: 0, Batch: 350, Loss: nan\n",
      "Epoch: 0, Batch: 351, Loss: nan\n",
      "Epoch: 0, Batch: 352, Loss: nan\n",
      "Epoch: 0, Batch: 353, Loss: nan\n",
      "Epoch: 0, Batch: 354, Loss: nan\n",
      "Epoch: 0, Batch: 355, Loss: nan\n",
      "Epoch: 0, Batch: 356, Loss: nan\n",
      "Epoch: 0, Batch: 357, Loss: nan\n",
      "Epoch: 0, Batch: 358, Loss: nan\n",
      "Epoch: 0, Batch: 359, Loss: nan\n",
      "Epoch: 0, Batch: 360, Loss: nan\n",
      "Epoch: 0, Batch: 361, Loss: nan\n",
      "Epoch: 0, Batch: 362, Loss: nan\n",
      "Epoch: 0, Batch: 363, Loss: nan\n",
      "Epoch: 0, Batch: 364, Loss: nan\n",
      "Epoch: 0, Batch: 365, Loss: nan\n",
      "Epoch: 0, Batch: 366, Loss: nan\n",
      "Epoch: 0, Batch: 367, Loss: nan\n",
      "Epoch: 0, Batch: 368, Loss: nan\n",
      "Epoch: 0, Batch: 369, Loss: nan\n",
      "Epoch: 0, Batch: 370, Loss: nan\n",
      "Epoch: 0, Batch: 371, Loss: nan\n",
      "Epoch: 0, Batch: 372, Loss: nan\n",
      "Epoch: 0, Batch: 373, Loss: nan\n",
      "Epoch: 0, Batch: 374, Loss: nan\n",
      "Epoch: 0, Batch: 375, Loss: nan\n",
      "Epoch: 0, Batch: 376, Loss: nan\n",
      "Epoch: 0, Batch: 377, Loss: nan\n",
      "Epoch: 0, Batch: 378, Loss: nan\n",
      "Epoch: 0, Batch: 379, Loss: nan\n",
      "Epoch: 0, Batch: 380, Loss: nan\n",
      "Epoch: 0, Batch: 381, Loss: nan\n",
      "Epoch: 0, Batch: 382, Loss: nan\n",
      "Epoch: 0, Batch: 383, Loss: nan\n",
      "Epoch: 0, Batch: 384, Loss: nan\n",
      "Epoch: 0, Batch: 385, Loss: nan\n",
      "Epoch: 0, Batch: 386, Loss: nan\n",
      "Epoch: 0, Batch: 387, Loss: nan\n",
      "Epoch: 0, Batch: 388, Loss: nan\n",
      "Epoch: 0, Batch: 389, Loss: nan\n",
      "Epoch: 0, Batch: 390, Loss: nan\n",
      "Epoch: 0, Batch: 391, Loss: nan\n",
      "Epoch: 0, Batch: 392, Loss: nan\n",
      "Epoch: 0, Batch: 393, Loss: nan\n",
      "Epoch: 0, Batch: 394, Loss: nan\n",
      "Epoch: 0, Batch: 395, Loss: nan\n",
      "Epoch: 0, Batch: 396, Loss: nan\n",
      "Epoch: 0, Batch: 397, Loss: nan\n",
      "Epoch: 0, Batch: 398, Loss: nan\n",
      "Epoch: 0, Batch: 399, Loss: nan\n",
      "Epoch: 0, Batch: 400, Loss: nan\n",
      "Epoch: 0, Batch: 401, Loss: nan\n",
      "Epoch: 0, Batch: 402, Loss: nan\n",
      "Epoch: 0, Batch: 403, Loss: nan\n",
      "Epoch: 0, Batch: 404, Loss: nan\n",
      "Epoch: 0, Batch: 405, Loss: nan\n",
      "Epoch: 0, Batch: 406, Loss: nan\n",
      "Epoch: 0, Batch: 407, Loss: nan\n",
      "Epoch: 0, Batch: 408, Loss: nan\n",
      "Epoch: 0, Batch: 409, Loss: nan\n",
      "Epoch: 0, Batch: 410, Loss: nan\n",
      "Epoch: 0, Batch: 411, Loss: nan\n",
      "Epoch: 0, Batch: 412, Loss: nan\n",
      "Epoch: 0, Batch: 413, Loss: nan\n",
      "Epoch: 0, Batch: 414, Loss: nan\n",
      "Epoch: 0, Batch: 415, Loss: nan\n",
      "Epoch: 0, Batch: 416, Loss: nan\n",
      "Epoch: 0, Batch: 417, Loss: nan\n",
      "Epoch: 0, Batch: 418, Loss: nan\n",
      "Epoch: 0, Batch: 419, Loss: nan\n",
      "Epoch: 0, Batch: 420, Loss: nan\n",
      "Epoch: 0, Batch: 421, Loss: nan\n",
      "Epoch: 0, Batch: 422, Loss: nan\n",
      "Epoch: 0, Batch: 423, Loss: nan\n",
      "Epoch: 0, Batch: 424, Loss: nan\n",
      "Epoch: 0, Batch: 425, Loss: nan\n",
      "Epoch: 0, Batch: 426, Loss: nan\n",
      "Epoch: 0, Batch: 427, Loss: nan\n",
      "Epoch: 0, Batch: 428, Loss: nan\n",
      "Epoch: 0, Batch: 429, Loss: nan\n",
      "Epoch: 0, Batch: 430, Loss: nan\n",
      "Epoch: 0, Batch: 431, Loss: nan\n",
      "Epoch: 0, Batch: 432, Loss: nan\n",
      "Epoch: 0, Batch: 433, Loss: nan\n",
      "Epoch: 0, Batch: 434, Loss: nan\n",
      "Epoch: 0, Batch: 435, Loss: nan\n",
      "Epoch: 0, Batch: 436, Loss: nan\n",
      "Epoch: 0, Batch: 437, Loss: nan\n",
      "Epoch: 0, Batch: 438, Loss: nan\n",
      "Epoch: 0, Batch: 439, Loss: nan\n",
      "Epoch: 0, Batch: 440, Loss: nan\n",
      "Epoch: 0, Batch: 441, Loss: nan\n",
      "Epoch: 0, Batch: 442, Loss: nan\n",
      "Epoch: 0, Batch: 443, Loss: nan\n",
      "Epoch: 0, Batch: 444, Loss: nan\n",
      "Epoch: 0, Batch: 445, Loss: nan\n",
      "Epoch: 0, Batch: 446, Loss: nan\n",
      "Epoch: 0, Batch: 447, Loss: nan\n",
      "Epoch: 0, Batch: 448, Loss: nan\n",
      "Epoch: 0, Batch: 449, Loss: nan\n",
      "Epoch: 0, Batch: 450, Loss: nan\n",
      "Epoch: 0, Batch: 451, Loss: nan\n",
      "Epoch: 0, Batch: 452, Loss: nan\n",
      "Epoch: 0, Batch: 453, Loss: nan\n",
      "Epoch: 0, Batch: 454, Loss: nan\n",
      "Epoch: 0, Batch: 455, Loss: nan\n",
      "Epoch: 0, Batch: 456, Loss: nan\n",
      "Epoch: 0, Batch: 457, Loss: nan\n",
      "Epoch: 0, Batch: 458, Loss: nan\n",
      "Epoch: 0, Batch: 459, Loss: nan\n",
      "Epoch: 0, Batch: 460, Loss: nan\n",
      "Epoch: 0, Batch: 461, Loss: nan\n",
      "Epoch: 0, Batch: 462, Loss: nan\n",
      "Epoch: 0, Batch: 463, Loss: nan\n",
      "Epoch: 0, Batch: 464, Loss: nan\n",
      "Epoch: 0, Batch: 465, Loss: nan\n",
      "Epoch: 0, Batch: 466, Loss: nan\n",
      "Epoch: 0, Batch: 467, Loss: nan\n",
      "Epoch: 0, Batch: 468, Loss: nan\n",
      "Epoch: 0, Batch: 469, Loss: nan\n",
      "Epoch: 0, Batch: 470, Loss: nan\n",
      "Epoch: 0, Batch: 471, Loss: nan\n",
      "Epoch: 0, Batch: 472, Loss: nan\n",
      "Epoch: 0, Batch: 473, Loss: nan\n",
      "Epoch: 0, Batch: 474, Loss: nan\n",
      "Epoch: 0, Batch: 475, Loss: nan\n",
      "Epoch: 0, Batch: 476, Loss: nan\n",
      "Epoch: 0, Batch: 477, Loss: nan\n",
      "Epoch: 0, Batch: 478, Loss: nan\n",
      "Epoch: 0, Batch: 479, Loss: nan\n",
      "Epoch: 0, Batch: 480, Loss: nan\n",
      "Epoch: 0, Batch: 481, Loss: nan\n",
      "Epoch: 0, Batch: 482, Loss: nan\n",
      "Epoch: 0, Batch: 483, Loss: nan\n",
      "Epoch: 0, Batch: 484, Loss: nan\n",
      "Epoch: 0, Batch: 485, Loss: nan\n",
      "Epoch: 0, Batch: 486, Loss: nan\n",
      "Epoch: 0, Batch: 487, Loss: nan\n",
      "Epoch: 0, Batch: 488, Loss: nan\n",
      "Epoch: 0, Batch: 489, Loss: nan\n",
      "Epoch: 0, Batch: 490, Loss: nan\n",
      "Epoch: 0, Batch: 491, Loss: nan\n",
      "Epoch: 0, Batch: 492, Loss: nan\n",
      "Epoch: 0, Batch: 493, Loss: nan\n",
      "Epoch: 0, Batch: 494, Loss: nan\n",
      "Epoch: 0, Batch: 495, Loss: nan\n",
      "Epoch: 0, Batch: 496, Loss: nan\n",
      "Epoch: 0, Batch: 497, Loss: nan\n",
      "Epoch: 0, Batch: 498, Loss: nan\n",
      "Epoch: 0, Batch: 499, Loss: nan\n",
      "Epoch: 0, Batch: 500, Loss: nan\n",
      "Epoch: 0, Batch: 501, Loss: nan\n",
      "Epoch: 0, Batch: 502, Loss: nan\n",
      "Epoch: 0, Batch: 503, Loss: nan\n",
      "Epoch: 0, Batch: 504, Loss: nan\n",
      "Epoch: 0, Batch: 505, Loss: nan\n",
      "Epoch: 0, Batch: 506, Loss: nan\n",
      "Epoch: 0, Batch: 507, Loss: nan\n",
      "Epoch: 0, Batch: 508, Loss: nan\n",
      "Epoch: 0, Batch: 509, Loss: nan\n",
      "Epoch: 0, Batch: 510, Loss: nan\n",
      "Epoch: 0, Batch: 511, Loss: nan\n",
      "Epoch: 0, Batch: 512, Loss: nan\n",
      "Epoch: 0, Batch: 513, Loss: nan\n",
      "Epoch: 0, Batch: 514, Loss: nan\n",
      "Epoch: 0, Batch: 515, Loss: nan\n",
      "Epoch: 0, Batch: 516, Loss: nan\n",
      "Epoch: 0, Batch: 517, Loss: nan\n",
      "Epoch: 0, Batch: 518, Loss: nan\n",
      "Epoch: 0, Batch: 519, Loss: nan\n",
      "Epoch: 0, Batch: 520, Loss: nan\n",
      "Epoch: 0, Batch: 521, Loss: nan\n",
      "Epoch: 0, Batch: 522, Loss: nan\n",
      "Epoch: 0, Batch: 523, Loss: nan\n",
      "Epoch: 0, Batch: 524, Loss: nan\n",
      "Epoch: 0, Batch: 525, Loss: nan\n",
      "Epoch: 0, Batch: 526, Loss: nan\n",
      "Epoch: 0, Batch: 527, Loss: nan\n",
      "Epoch: 0, Batch: 528, Loss: nan\n",
      "Epoch: 0, Batch: 529, Loss: nan\n",
      "Epoch: 0, Batch: 530, Loss: nan\n",
      "Epoch: 0, Batch: 531, Loss: nan\n",
      "Epoch: 0, Batch: 532, Loss: nan\n",
      "Epoch: 0, Batch: 533, Loss: nan\n",
      "Epoch: 0, Batch: 534, Loss: nan\n",
      "Epoch: 0, Batch: 535, Loss: nan\n",
      "Epoch: 0, Batch: 536, Loss: nan\n",
      "Epoch: 0, Batch: 537, Loss: nan\n",
      "Epoch: 0, Batch: 538, Loss: nan\n",
      "Epoch: 0, Batch: 539, Loss: nan\n",
      "Epoch: 0, Batch: 540, Loss: nan\n",
      "Epoch: 0, Batch: 541, Loss: nan\n",
      "Epoch: 0, Batch: 542, Loss: nan\n",
      "Epoch: 0, Batch: 543, Loss: nan\n",
      "Epoch: 0, Batch: 544, Loss: nan\n",
      "Epoch: 0, Batch: 545, Loss: nan\n",
      "Epoch: 0, Batch: 546, Loss: nan\n",
      "Epoch: 0, Batch: 547, Loss: nan\n",
      "Epoch: 0, Batch: 548, Loss: nan\n",
      "Epoch: 0, Batch: 549, Loss: nan\n",
      "Epoch: 0, Batch: 550, Loss: nan\n",
      "Epoch: 0, Batch: 551, Loss: nan\n",
      "Epoch: 0, Batch: 552, Loss: nan\n",
      "Epoch: 0, Batch: 553, Loss: nan\n",
      "Epoch: 0, Batch: 554, Loss: nan\n",
      "Epoch: 0, Batch: 555, Loss: nan\n",
      "Epoch: 0, Batch: 556, Loss: nan\n",
      "Epoch: 0, Batch: 557, Loss: nan\n",
      "Epoch: 0, Batch: 558, Loss: nan\n",
      "Epoch: 0, Batch: 559, Loss: nan\n",
      "Epoch: 0, Batch: 560, Loss: nan\n",
      "Epoch: 0, Batch: 561, Loss: nan\n",
      "Epoch: 0, Batch: 562, Loss: nan\n",
      "Epoch: 0, Batch: 563, Loss: nan\n",
      "Epoch: 0, Batch: 564, Loss: nan\n",
      "Epoch: 0, Batch: 565, Loss: nan\n",
      "Epoch: 0, Batch: 566, Loss: nan\n",
      "Epoch: 0, Batch: 567, Loss: nan\n",
      "Epoch: 0, Batch: 568, Loss: nan\n",
      "Epoch: 0, Batch: 569, Loss: nan\n",
      "Epoch: 0, Batch: 570, Loss: nan\n",
      "Epoch: 0, Batch: 571, Loss: nan\n",
      "Epoch: 0, Batch: 572, Loss: nan\n",
      "Epoch: 0, Batch: 573, Loss: nan\n",
      "Epoch: 0, Batch: 574, Loss: nan\n",
      "Epoch: 0, Batch: 575, Loss: nan\n",
      "Epoch: 0, Batch: 576, Loss: nan\n",
      "Epoch: 0, Batch: 577, Loss: nan\n",
      "Epoch: 0, Batch: 578, Loss: nan\n",
      "Epoch: 0, Batch: 579, Loss: nan\n",
      "Epoch: 0, Batch: 580, Loss: nan\n",
      "Epoch: 0, Batch: 581, Loss: nan\n",
      "Epoch: 0, Batch: 582, Loss: nan\n",
      "Epoch: 0, Batch: 583, Loss: nan\n",
      "Epoch: 0, Batch: 584, Loss: nan\n",
      "Epoch: 0, Batch: 585, Loss: nan\n",
      "Epoch: 0, Batch: 586, Loss: nan\n",
      "Epoch: 0, Batch: 587, Loss: nan\n",
      "Epoch: 0, Batch: 588, Loss: nan\n",
      "Epoch: 0, Batch: 589, Loss: nan\n",
      "Epoch: 0, Batch: 590, Loss: nan\n",
      "Epoch: 0, Batch: 591, Loss: nan\n",
      "Epoch: 0, Batch: 592, Loss: nan\n",
      "Epoch: 0, Batch: 593, Loss: nan\n",
      "Epoch: 0, Batch: 594, Loss: nan\n",
      "Epoch: 0, Batch: 595, Loss: nan\n",
      "Epoch: 0, Batch: 596, Loss: nan\n",
      "Epoch: 0, Batch: 597, Loss: nan\n",
      "Epoch: 0, Batch: 598, Loss: nan\n",
      "Epoch: 0, Batch: 599, Loss: nan\n",
      "Epoch: 0, Batch: 600, Loss: nan\n",
      "Epoch: 0, Batch: 601, Loss: nan\n",
      "Epoch: 0, Batch: 602, Loss: nan\n",
      "Epoch: 0, Batch: 603, Loss: nan\n",
      "Epoch: 0, Batch: 604, Loss: nan\n",
      "Epoch: 0, Batch: 605, Loss: nan\n",
      "Epoch: 0, Batch: 606, Loss: nan\n",
      "Epoch: 0, Batch: 607, Loss: nan\n",
      "Epoch: 0, Batch: 608, Loss: nan\n",
      "Epoch: 0, Batch: 609, Loss: nan\n",
      "Epoch: 0, Batch: 610, Loss: nan\n",
      "Epoch: 0, Batch: 611, Loss: nan\n",
      "Epoch: 0, Batch: 612, Loss: nan\n",
      "Epoch: 0, Batch: 613, Loss: nan\n",
      "Epoch: 0, Batch: 614, Loss: nan\n",
      "Epoch: 0, Batch: 615, Loss: nan\n",
      "Epoch: 0, Batch: 616, Loss: nan\n",
      "Epoch: 0, Batch: 617, Loss: nan\n",
      "Epoch: 0, Batch: 618, Loss: nan\n",
      "Epoch: 0, Batch: 619, Loss: nan\n",
      "Epoch: 0, Batch: 620, Loss: nan\n",
      "Epoch: 0, Batch: 621, Loss: nan\n",
      "Epoch: 0, Batch: 622, Loss: nan\n",
      "Epoch: 0, Batch: 623, Loss: nan\n",
      "Epoch: 0, Batch: 624, Loss: nan\n",
      "Epoch: 0, Batch: 625, Loss: nan\n",
      "Epoch: 0, Batch: 626, Loss: nan\n",
      "Epoch: 0, Batch: 627, Loss: nan\n",
      "Epoch: 0, Batch: 628, Loss: nan\n",
      "Epoch: 0, Batch: 629, Loss: nan\n",
      "Epoch: 0, Batch: 630, Loss: nan\n",
      "Epoch: 0, Batch: 631, Loss: nan\n",
      "Epoch: 0, Batch: 632, Loss: nan\n",
      "Epoch: 0, Batch: 633, Loss: nan\n",
      "Epoch: 0, Batch: 634, Loss: nan\n",
      "Epoch: 0, Batch: 635, Loss: nan\n",
      "Epoch: 0, Batch: 636, Loss: nan\n",
      "Epoch: 0, Batch: 637, Loss: nan\n",
      "Epoch: 0, Batch: 638, Loss: nan\n",
      "Epoch: 0, Batch: 639, Loss: nan\n",
      "Epoch: 0, Batch: 640, Loss: nan\n",
      "Epoch: 0, Batch: 641, Loss: nan\n",
      "Epoch: 0, Batch: 642, Loss: nan\n",
      "Epoch: 0, Batch: 643, Loss: nan\n",
      "Epoch: 0, Batch: 644, Loss: nan\n",
      "Epoch: 0, Batch: 645, Loss: nan\n",
      "Epoch: 0, Batch: 646, Loss: nan\n",
      "Epoch: 0, Batch: 647, Loss: nan\n",
      "Epoch: 0, Batch: 648, Loss: nan\n",
      "Epoch: 0, Batch: 649, Loss: nan\n",
      "Epoch: 0, Batch: 650, Loss: nan\n",
      "Epoch: 0, Batch: 651, Loss: nan\n",
      "Epoch: 0, Batch: 652, Loss: nan\n",
      "Epoch: 0, Batch: 653, Loss: nan\n",
      "Epoch: 0, Batch: 654, Loss: nan\n",
      "Epoch: 0, Batch: 655, Loss: nan\n",
      "Epoch: 0, Batch: 656, Loss: nan\n",
      "Epoch: 0, Batch: 657, Loss: nan\n",
      "Epoch: 0, Batch: 658, Loss: nan\n",
      "Epoch: 0, Batch: 659, Loss: nan\n",
      "Epoch: 0, Batch: 660, Loss: nan\n",
      "Epoch: 0, Batch: 661, Loss: nan\n",
      "Epoch: 0, Batch: 662, Loss: nan\n",
      "Epoch: 0, Batch: 663, Loss: nan\n",
      "Epoch: 0, Batch: 664, Loss: nan\n",
      "Epoch: 0, Batch: 665, Loss: nan\n",
      "Epoch: 0, Batch: 666, Loss: nan\n",
      "Epoch: 0, Batch: 667, Loss: nan\n",
      "Epoch: 0, Batch: 668, Loss: nan\n",
      "Epoch: 0, Batch: 669, Loss: nan\n",
      "Epoch: 0, Batch: 670, Loss: nan\n",
      "Epoch: 0, Batch: 671, Loss: nan\n",
      "Epoch: 0, Batch: 672, Loss: nan\n",
      "Epoch: 0, Batch: 673, Loss: nan\n",
      "Epoch: 0, Batch: 674, Loss: nan\n",
      "Epoch: 0, Batch: 675, Loss: nan\n",
      "Epoch: 0, Batch: 676, Loss: nan\n",
      "Epoch: 0, Batch: 677, Loss: nan\n",
      "Epoch: 0, Batch: 678, Loss: nan\n",
      "Epoch: 0, Batch: 679, Loss: nan\n",
      "Epoch: 0, Batch: 680, Loss: nan\n",
      "Epoch: 0, Batch: 681, Loss: nan\n",
      "Epoch: 0, Batch: 682, Loss: nan\n",
      "Epoch: 0, Batch: 683, Loss: nan\n",
      "Epoch: 0, Batch: 684, Loss: nan\n",
      "Epoch: 0, Batch: 685, Loss: nan\n",
      "Epoch: 0, Batch: 686, Loss: nan\n",
      "Epoch: 0, Batch: 687, Loss: nan\n",
      "Epoch: 0, Batch: 688, Loss: nan\n",
      "Epoch: 0, Batch: 689, Loss: nan\n",
      "Epoch: 0, Batch: 690, Loss: nan\n",
      "Epoch: 0, Batch: 691, Loss: nan\n",
      "Epoch: 0, Batch: 692, Loss: nan\n",
      "Epoch: 0, Batch: 693, Loss: nan\n",
      "Epoch: 0, Batch: 694, Loss: nan\n",
      "Epoch: 0, Batch: 695, Loss: nan\n",
      "Epoch: 0, Batch: 696, Loss: nan\n",
      "Epoch: 0, Batch: 697, Loss: nan\n",
      "Epoch: 0, Batch: 698, Loss: nan\n",
      "Epoch: 0, Batch: 699, Loss: nan\n",
      "Epoch: 0, Batch: 700, Loss: nan\n",
      "Epoch: 0, Batch: 701, Loss: nan\n",
      "Epoch: 0, Batch: 702, Loss: nan\n",
      "Epoch: 0, Batch: 703, Loss: nan\n",
      "Epoch: 0, Batch: 704, Loss: nan\n",
      "Epoch: 0, Batch: 705, Loss: nan\n",
      "Epoch: 0, Batch: 706, Loss: nan\n",
      "Epoch: 0, Batch: 707, Loss: nan\n",
      "Epoch: 0, Batch: 708, Loss: nan\n",
      "Epoch: 0, Batch: 709, Loss: nan\n",
      "Epoch: 0, Batch: 710, Loss: nan\n",
      "Epoch: 0, Batch: 711, Loss: nan\n",
      "Epoch: 0, Batch: 712, Loss: nan\n",
      "Epoch: 0, Batch: 713, Loss: nan\n",
      "Epoch: 0, Batch: 714, Loss: nan\n",
      "Epoch: 0, Batch: 715, Loss: nan\n",
      "Epoch: 0, Batch: 716, Loss: nan\n",
      "Epoch: 0, Batch: 717, Loss: nan\n",
      "Epoch: 0, Batch: 718, Loss: nan\n",
      "Epoch: 0, Batch: 719, Loss: nan\n",
      "Epoch: 0, Batch: 720, Loss: nan\n",
      "Epoch: 0, Batch: 721, Loss: nan\n",
      "Epoch: 0, Batch: 722, Loss: nan\n",
      "Epoch: 0, Batch: 723, Loss: nan\n",
      "Epoch: 0, Batch: 724, Loss: nan\n",
      "Epoch: 0, Batch: 725, Loss: nan\n",
      "Epoch: 0, Batch: 726, Loss: nan\n",
      "Epoch: 0, Batch: 727, Loss: nan\n",
      "Epoch: 0, Batch: 728, Loss: nan\n",
      "Epoch: 0, Batch: 729, Loss: nan\n",
      "Epoch: 0, Batch: 730, Loss: nan\n",
      "Epoch: 0, Batch: 731, Loss: nan\n",
      "Epoch: 0, Batch: 732, Loss: nan\n",
      "Epoch: 0, Batch: 733, Loss: nan\n",
      "Epoch: 0, Batch: 734, Loss: nan\n",
      "Epoch: 0, Batch: 735, Loss: nan\n",
      "Epoch: 0, Batch: 736, Loss: nan\n",
      "Epoch: 0, Batch: 737, Loss: nan\n",
      "Epoch: 0, Batch: 738, Loss: nan\n",
      "Epoch: 0, Batch: 739, Loss: nan\n",
      "Epoch: 0, Batch: 740, Loss: nan\n",
      "Epoch: 0, Batch: 741, Loss: nan\n",
      "Epoch: 0, Batch: 742, Loss: nan\n",
      "Epoch: 0, Batch: 743, Loss: nan\n",
      "Epoch: 0, Batch: 744, Loss: nan\n",
      "Epoch: 0, Batch: 745, Loss: nan\n",
      "Epoch: 0, Batch: 746, Loss: nan\n",
      "Epoch: 0, Batch: 747, Loss: nan\n",
      "Epoch: 0, Batch: 748, Loss: nan\n",
      "Epoch: 0, Batch: 749, Loss: nan\n",
      "Epoch: 0, Batch: 750, Loss: nan\n",
      "Epoch: 0, Batch: 751, Loss: nan\n",
      "Epoch: 0, Batch: 752, Loss: nan\n",
      "Epoch: 0, Batch: 753, Loss: nan\n",
      "Epoch: 0, Batch: 754, Loss: nan\n",
      "Epoch: 0, Batch: 755, Loss: nan\n",
      "Epoch: 0, Batch: 756, Loss: nan\n",
      "Epoch: 0, Batch: 757, Loss: nan\n",
      "Epoch: 0, Batch: 758, Loss: nan\n",
      "Epoch: 0, Batch: 759, Loss: nan\n",
      "Epoch: 0, Batch: 760, Loss: nan\n",
      "Epoch: 0, Batch: 761, Loss: nan\n",
      "Epoch: 0, Batch: 762, Loss: nan\n",
      "Epoch: 0, Batch: 763, Loss: nan\n",
      "Epoch: 0, Batch: 764, Loss: nan\n",
      "Epoch: 0, Batch: 765, Loss: nan\n",
      "Epoch: 0, Batch: 766, Loss: nan\n",
      "Epoch: 0, Batch: 767, Loss: nan\n",
      "Epoch: 0, Batch: 768, Loss: nan\n",
      "Epoch: 0, Batch: 769, Loss: nan\n",
      "Epoch: 0, Batch: 770, Loss: nan\n",
      "Epoch: 0, Batch: 771, Loss: nan\n",
      "Epoch: 0, Batch: 772, Loss: nan\n",
      "Epoch: 0, Batch: 773, Loss: nan\n",
      "Epoch: 0, Batch: 774, Loss: nan\n",
      "Epoch: 0, Batch: 775, Loss: nan\n",
      "Epoch: 0, Batch: 776, Loss: nan\n",
      "Epoch: 0, Batch: 777, Loss: nan\n",
      "Epoch: 0, Batch: 778, Loss: nan\n",
      "Epoch: 0, Batch: 779, Loss: nan\n",
      "Epoch: 0, Batch: 780, Loss: nan\n",
      "Epoch: 0, Batch: 781, Loss: nan\n",
      "Epoch: 0, Batch: 782, Loss: nan\n",
      "Epoch: 0, Batch: 783, Loss: nan\n",
      "Epoch: 0, Batch: 784, Loss: nan\n",
      "Epoch: 0, Batch: 785, Loss: nan\n",
      "Epoch: 0, Batch: 786, Loss: nan\n",
      "Epoch: 0, Batch: 787, Loss: nan\n",
      "Epoch: 0, Batch: 788, Loss: nan\n",
      "Epoch: 0, Batch: 789, Loss: nan\n",
      "Epoch: 0, Batch: 790, Loss: nan\n",
      "Epoch: 0, Batch: 791, Loss: nan\n",
      "Epoch: 0, Batch: 792, Loss: nan\n",
      "Epoch: 0, Batch: 793, Loss: nan\n",
      "Epoch: 0, Batch: 794, Loss: nan\n",
      "Epoch: 0, Batch: 795, Loss: nan\n",
      "Epoch: 0, Batch: 796, Loss: nan\n",
      "Epoch: 0, Batch: 797, Loss: nan\n",
      "Epoch: 0, Batch: 798, Loss: nan\n",
      "Epoch: 0, Batch: 799, Loss: nan\n",
      "Epoch: 0, Batch: 800, Loss: nan\n",
      "Epoch: 0, Batch: 801, Loss: nan\n",
      "Epoch: 0, Batch: 802, Loss: nan\n",
      "Epoch: 0, Batch: 803, Loss: nan\n",
      "Epoch: 0, Batch: 804, Loss: nan\n",
      "Epoch: 0, Batch: 805, Loss: nan\n",
      "Epoch: 0, Batch: 806, Loss: nan\n",
      "Epoch: 0, Batch: 807, Loss: nan\n",
      "Epoch: 0, Batch: 808, Loss: nan\n",
      "Epoch: 0, Batch: 809, Loss: nan\n",
      "Epoch: 0, Batch: 810, Loss: nan\n",
      "Epoch: 0, Batch: 811, Loss: nan\n",
      "Epoch: 0, Batch: 812, Loss: nan\n",
      "Epoch: 0, Batch: 813, Loss: nan\n",
      "Epoch: 0, Batch: 814, Loss: nan\n",
      "Epoch: 0, Batch: 815, Loss: nan\n",
      "Epoch: 0, Batch: 816, Loss: nan\n",
      "Epoch: 0, Batch: 817, Loss: nan\n",
      "Epoch: 0, Batch: 818, Loss: nan\n",
      "Epoch: 0, Batch: 819, Loss: nan\n",
      "Epoch: 0, Batch: 820, Loss: nan\n",
      "Epoch: 0, Batch: 821, Loss: nan\n",
      "Epoch: 0, Batch: 822, Loss: nan\n",
      "Epoch: 0, Batch: 823, Loss: nan\n",
      "Epoch: 0, Batch: 824, Loss: nan\n",
      "Epoch: 0, Batch: 825, Loss: nan\n",
      "Epoch: 0, Batch: 826, Loss: nan\n",
      "Epoch: 0, Batch: 827, Loss: nan\n",
      "Epoch: 0, Batch: 828, Loss: nan\n",
      "Epoch: 0, Batch: 829, Loss: nan\n",
      "Epoch: 0, Batch: 830, Loss: nan\n",
      "Epoch: 0, Batch: 831, Loss: nan\n",
      "Epoch: 0, Batch: 832, Loss: nan\n",
      "Epoch: 0, Batch: 833, Loss: nan\n",
      "Epoch: 0, Batch: 834, Loss: nan\n",
      "Epoch: 0, Batch: 835, Loss: nan\n",
      "Epoch: 0, Batch: 836, Loss: nan\n",
      "Epoch: 0, Batch: 837, Loss: nan\n",
      "Epoch: 0, Batch: 838, Loss: nan\n",
      "Epoch: 0, Batch: 839, Loss: nan\n",
      "Epoch: 0, Batch: 840, Loss: nan\n",
      "Epoch: 0, Batch: 841, Loss: nan\n",
      "Epoch: 0, Batch: 842, Loss: nan\n",
      "Epoch: 0, Batch: 843, Loss: nan\n",
      "Epoch: 0, Batch: 844, Loss: nan\n",
      "Epoch: 0, Batch: 845, Loss: nan\n",
      "Epoch: 0, Batch: 846, Loss: nan\n",
      "Epoch: 0, Batch: 847, Loss: nan\n",
      "Epoch: 0, Batch: 848, Loss: nan\n",
      "Epoch: 0, Batch: 849, Loss: nan\n",
      "Epoch: 0, Batch: 850, Loss: nan\n",
      "Epoch: 0, Batch: 851, Loss: nan\n",
      "Epoch: 0, Batch: 852, Loss: nan\n",
      "Epoch: 0, Batch: 853, Loss: nan\n",
      "Epoch: 0, Batch: 854, Loss: nan\n",
      "Epoch: 0, Batch: 855, Loss: nan\n",
      "Epoch: 0, Batch: 856, Loss: nan\n",
      "Epoch: 0, Batch: 857, Loss: nan\n",
      "Epoch: 0, Batch: 858, Loss: nan\n",
      "Epoch: 0, Batch: 859, Loss: nan\n",
      "Epoch: 0, Batch: 860, Loss: nan\n",
      "Epoch: 0, Batch: 861, Loss: nan\n",
      "Epoch: 0, Batch: 862, Loss: nan\n",
      "Epoch: 0, Batch: 863, Loss: nan\n",
      "Epoch: 0, Batch: 864, Loss: nan\n",
      "Epoch: 0, Batch: 865, Loss: nan\n",
      "Epoch: 0, Batch: 866, Loss: nan\n",
      "Epoch: 0, Batch: 867, Loss: nan\n",
      "Epoch: 0, Batch: 868, Loss: nan\n",
      "Epoch: 0, Batch: 869, Loss: nan\n",
      "Epoch: 0, Batch: 870, Loss: nan\n",
      "Epoch: 0, Batch: 871, Loss: nan\n",
      "Epoch: 0, Batch: 872, Loss: nan\n",
      "Epoch: 0, Batch: 873, Loss: nan\n",
      "Epoch: 0, Batch: 874, Loss: nan\n",
      "Epoch: 0, Batch: 875, Loss: nan\n",
      "Epoch: 0, Batch: 876, Loss: nan\n",
      "Epoch: 0, Batch: 877, Loss: nan\n",
      "Epoch: 0, Batch: 878, Loss: nan\n",
      "Epoch: 0, Batch: 879, Loss: nan\n",
      "Epoch: 0, Batch: 880, Loss: nan\n",
      "Epoch: 0, Batch: 881, Loss: nan\n",
      "Epoch: 0, Batch: 882, Loss: nan\n",
      "Epoch: 0, Batch: 883, Loss: nan\n",
      "Epoch: 0, Batch: 884, Loss: nan\n",
      "Epoch: 0, Batch: 885, Loss: nan\n",
      "Epoch: 0, Batch: 886, Loss: nan\n",
      "Epoch: 0, Batch: 887, Loss: nan\n",
      "Epoch: 0, Batch: 888, Loss: nan\n",
      "Epoch: 0, Batch: 889, Loss: nan\n",
      "Epoch: 0, Batch: 890, Loss: nan\n",
      "Epoch: 0, Batch: 891, Loss: nan\n",
      "Epoch: 0, Batch: 892, Loss: nan\n",
      "Epoch: 0, Batch: 893, Loss: nan\n",
      "Epoch: 0, Batch: 894, Loss: nan\n",
      "Epoch: 0, Batch: 895, Loss: nan\n",
      "Epoch: 0, Batch: 896, Loss: nan\n",
      "Epoch: 0, Batch: 897, Loss: nan\n",
      "Epoch: 0, Batch: 898, Loss: nan\n",
      "Epoch: 0, Batch: 899, Loss: nan\n",
      "Epoch: 0, Batch: 900, Loss: nan\n",
      "Epoch: 0, Batch: 901, Loss: nan\n",
      "Epoch: 0, Batch: 902, Loss: nan\n",
      "Epoch: 0, Batch: 903, Loss: nan\n",
      "Epoch: 0, Batch: 904, Loss: nan\n",
      "Epoch: 0, Batch: 905, Loss: nan\n",
      "Epoch: 0, Batch: 906, Loss: nan\n",
      "Epoch: 0, Batch: 907, Loss: nan\n",
      "Epoch: 0, Batch: 908, Loss: nan\n",
      "Epoch: 0, Batch: 909, Loss: nan\n",
      "Epoch: 0, Batch: 910, Loss: nan\n",
      "Epoch: 0, Batch: 911, Loss: nan\n",
      "Epoch: 0, Batch: 912, Loss: nan\n",
      "Epoch: 0, Batch: 913, Loss: nan\n",
      "Epoch: 0, Batch: 914, Loss: nan\n",
      "Epoch: 0, Batch: 915, Loss: nan\n",
      "Epoch: 0, Batch: 916, Loss: nan\n",
      "Epoch: 0, Batch: 917, Loss: nan\n",
      "Epoch: 0, Batch: 918, Loss: nan\n",
      "Epoch: 0, Batch: 919, Loss: nan\n",
      "Epoch: 0, Batch: 920, Loss: nan\n",
      "Epoch: 0, Batch: 921, Loss: nan\n",
      "Epoch: 0, Batch: 922, Loss: nan\n",
      "Epoch: 0, Batch: 923, Loss: nan\n",
      "Epoch: 0, Batch: 924, Loss: nan\n",
      "Epoch: 0, Batch: 925, Loss: nan\n",
      "Epoch: 0, Batch: 926, Loss: nan\n",
      "Epoch: 0, Batch: 927, Loss: nan\n",
      "Epoch: 0, Batch: 928, Loss: nan\n",
      "Epoch: 0, Batch: 929, Loss: nan\n",
      "Epoch: 0, Batch: 930, Loss: nan\n",
      "Epoch: 0, Batch: 931, Loss: nan\n",
      "Epoch: 0, Batch: 932, Loss: nan\n",
      "Epoch: 0, Batch: 933, Loss: nan\n",
      "Epoch: 0, Batch: 934, Loss: nan\n",
      "Epoch: 0, Batch: 935, Loss: nan\n",
      "Epoch: 0, Batch: 936, Loss: nan\n",
      "Epoch: 0, Batch: 937, Loss: nan\n",
      "Epoch: 0, Batch: 938, Loss: nan\n",
      "Epoch: 0, Batch: 939, Loss: nan\n",
      "Epoch: 0, Batch: 940, Loss: nan\n",
      "Epoch: 0, Batch: 941, Loss: nan\n",
      "Epoch: 0, Batch: 942, Loss: nan\n",
      "Epoch: 0, Batch: 943, Loss: nan\n",
      "Epoch: 0, Batch: 944, Loss: nan\n",
      "Epoch: 0, Batch: 945, Loss: nan\n",
      "Epoch: 0, Batch: 946, Loss: nan\n",
      "Epoch: 0, Batch: 947, Loss: nan\n",
      "Epoch: 0, Batch: 948, Loss: nan\n",
      "Epoch: 0, Batch: 949, Loss: nan\n",
      "Epoch: 0, Batch: 950, Loss: nan\n",
      "Epoch: 0, Batch: 951, Loss: nan\n",
      "Epoch: 0, Batch: 952, Loss: nan\n",
      "Epoch: 0, Batch: 953, Loss: nan\n",
      "Epoch: 0, Batch: 954, Loss: nan\n",
      "Epoch: 0, Batch: 955, Loss: nan\n",
      "Epoch: 0, Batch: 956, Loss: nan\n",
      "Epoch: 0, Batch: 957, Loss: nan\n",
      "Epoch: 0, Batch: 958, Loss: nan\n",
      "Epoch: 0, Batch: 959, Loss: nan\n",
      "Epoch: 0, Batch: 960, Loss: nan\n",
      "Epoch: 0, Batch: 961, Loss: nan\n",
      "Epoch: 0, Batch: 962, Loss: nan\n",
      "Epoch: 0, Batch: 963, Loss: nan\n",
      "Epoch: 0, Batch: 964, Loss: nan\n",
      "Epoch: 0, Batch: 965, Loss: nan\n",
      "Epoch: 0, Batch: 966, Loss: nan\n",
      "Epoch: 0, Batch: 967, Loss: nan\n",
      "Epoch: 0, Batch: 968, Loss: nan\n",
      "Epoch: 0, Batch: 969, Loss: nan\n",
      "Epoch: 0, Batch: 970, Loss: nan\n",
      "Epoch: 0, Batch: 971, Loss: nan\n",
      "Epoch: 0, Batch: 972, Loss: nan\n",
      "Epoch: 0, Batch: 973, Loss: nan\n",
      "Epoch: 0, Batch: 974, Loss: nan\n",
      "Epoch: 0, Batch: 975, Loss: nan\n",
      "Epoch: 0, Batch: 976, Loss: nan\n",
      "Epoch: 0, Batch: 977, Loss: nan\n",
      "Epoch: 0, Batch: 978, Loss: nan\n",
      "Epoch: 0, Batch: 979, Loss: nan\n",
      "Epoch: 0, Batch: 980, Loss: nan\n",
      "Epoch: 0, Batch: 981, Loss: nan\n",
      "Epoch: 0, Batch: 982, Loss: nan\n",
      "Epoch: 0, Batch: 983, Loss: nan\n",
      "Epoch: 0, Batch: 984, Loss: nan\n",
      "Epoch: 0, Batch: 985, Loss: nan\n",
      "Epoch: 0, Batch: 986, Loss: nan\n",
      "Epoch: 0, Batch: 987, Loss: nan\n",
      "Epoch: 0, Batch: 988, Loss: nan\n",
      "Epoch: 0, Batch: 989, Loss: nan\n",
      "Epoch: 0, Batch: 990, Loss: nan\n",
      "Epoch: 0, Batch: 991, Loss: nan\n",
      "Epoch: 0, Batch: 992, Loss: nan\n",
      "Epoch: 0, Batch: 993, Loss: nan\n",
      "Epoch: 0, Batch: 994, Loss: nan\n",
      "Epoch: 0, Batch: 995, Loss: nan\n",
      "Epoch: 0, Batch: 996, Loss: nan\n",
      "Epoch: 0, Batch: 997, Loss: nan\n",
      "Epoch: 0, Batch: 998, Loss: nan\n",
      "Epoch: 0, Batch: 999, Loss: nan\n",
      "Epoch: 0, Batch: 1000, Loss: nan\n",
      "Epoch: 0, Batch: 1001, Loss: nan\n",
      "Epoch: 0, Batch: 1002, Loss: nan\n",
      "Epoch: 0, Batch: 1003, Loss: nan\n",
      "Epoch: 0, Batch: 1004, Loss: nan\n",
      "Epoch: 0, Batch: 1005, Loss: nan\n",
      "Epoch: 0, Batch: 1006, Loss: nan\n",
      "Epoch: 0, Batch: 1007, Loss: nan\n",
      "Epoch: 0, Batch: 1008, Loss: nan\n",
      "Epoch: 0, Batch: 1009, Loss: nan\n",
      "Epoch: 0, Batch: 1010, Loss: nan\n",
      "Epoch: 0, Batch: 1011, Loss: nan\n",
      "Epoch: 0, Batch: 1012, Loss: nan\n",
      "Epoch: 0, Batch: 1013, Loss: nan\n",
      "Epoch: 0, Batch: 1014, Loss: nan\n",
      "Epoch: 0, Batch: 1015, Loss: nan\n",
      "Epoch: 0, Batch: 1016, Loss: nan\n",
      "Epoch: 0, Batch: 1017, Loss: nan\n",
      "Epoch: 0, Batch: 1018, Loss: nan\n",
      "Epoch: 0, Batch: 1019, Loss: nan\n",
      "Epoch: 0, Batch: 1020, Loss: nan\n",
      "Epoch: 0, Batch: 1021, Loss: nan\n",
      "Epoch: 0, Batch: 1022, Loss: nan\n",
      "Epoch: 0, Batch: 1023, Loss: nan\n",
      "Epoch: 0, Batch: 1024, Loss: nan\n",
      "Epoch: 0, Batch: 1025, Loss: nan\n",
      "Epoch: 0, Batch: 1026, Loss: nan\n",
      "Epoch: 0, Batch: 1027, Loss: nan\n",
      "Epoch: 0, Batch: 1028, Loss: nan\n",
      "Epoch: 0, Batch: 1029, Loss: nan\n",
      "Epoch: 0, Batch: 1030, Loss: nan\n",
      "Epoch: 0, Batch: 1031, Loss: nan\n",
      "Epoch: 0, Batch: 1032, Loss: nan\n",
      "Epoch: 0, Batch: 1033, Loss: nan\n",
      "Epoch: 0, Batch: 1034, Loss: nan\n",
      "Epoch: 0, Batch: 1035, Loss: nan\n",
      "Epoch: 0, Batch: 1036, Loss: nan\n",
      "Epoch: 0, Batch: 1037, Loss: nan\n",
      "Epoch: 0, Batch: 1038, Loss: nan\n",
      "Epoch: 0, Batch: 1039, Loss: nan\n",
      "Epoch: 0, Batch: 1040, Loss: nan\n",
      "Epoch: 0, Batch: 1041, Loss: nan\n",
      "Epoch: 0, Batch: 1042, Loss: nan\n",
      "Epoch: 0, Batch: 1043, Loss: nan\n",
      "Epoch: 0, Batch: 1044, Loss: nan\n",
      "Epoch: 0, Batch: 1045, Loss: nan\n",
      "Epoch: 0, Batch: 1046, Loss: nan\n",
      "Epoch: 0, Batch: 1047, Loss: nan\n",
      "Epoch: 0, Batch: 1048, Loss: nan\n",
      "Epoch: 0, Batch: 1049, Loss: nan\n",
      "Epoch: 0, Batch: 1050, Loss: nan\n",
      "Epoch: 0, Batch: 1051, Loss: nan\n",
      "Epoch: 0, Batch: 1052, Loss: nan\n",
      "Epoch: 0, Batch: 1053, Loss: nan\n",
      "Epoch: 0, Batch: 1054, Loss: nan\n",
      "Epoch: 0, Batch: 1055, Loss: nan\n",
      "Epoch: 0, Batch: 1056, Loss: nan\n",
      "Epoch: 0, Batch: 1057, Loss: nan\n",
      "Epoch: 0, Batch: 1058, Loss: nan\n",
      "Epoch: 0, Batch: 1059, Loss: nan\n",
      "Epoch: 0, Batch: 1060, Loss: nan\n",
      "Epoch: 0, Batch: 1061, Loss: nan\n",
      "Epoch: 0, Batch: 1062, Loss: nan\n",
      "Epoch: 0, Batch: 1063, Loss: nan\n",
      "Epoch: 0, Batch: 1064, Loss: nan\n",
      "Epoch: 0, Batch: 1065, Loss: nan\n",
      "Epoch: 0, Batch: 1066, Loss: nan\n",
      "Epoch: 0, Batch: 1067, Loss: nan\n",
      "Epoch: 0, Batch: 1068, Loss: nan\n",
      "Epoch: 0, Batch: 1069, Loss: nan\n",
      "Epoch: 0, Batch: 1070, Loss: nan\n",
      "Epoch: 0, Batch: 1071, Loss: nan\n",
      "Epoch: 0, Batch: 1072, Loss: nan\n",
      "Epoch: 0, Batch: 1073, Loss: nan\n",
      "Epoch: 0, Batch: 1074, Loss: nan\n",
      "Epoch: 0, Batch: 1075, Loss: nan\n",
      "Epoch: 0, Batch: 1076, Loss: nan\n",
      "Epoch: 0, Batch: 1077, Loss: nan\n",
      "Epoch: 0, Batch: 1078, Loss: nan\n",
      "Epoch: 0, Batch: 1079, Loss: nan\n",
      "Epoch: 0, Batch: 1080, Loss: nan\n",
      "Epoch: 0, Batch: 1081, Loss: nan\n",
      "Epoch: 0, Batch: 1082, Loss: nan\n",
      "Epoch: 0, Batch: 1083, Loss: nan\n",
      "Epoch: 0, Batch: 1084, Loss: nan\n",
      "Epoch: 0, Batch: 1085, Loss: nan\n",
      "Epoch: 0, Batch: 1086, Loss: nan\n",
      "Epoch: 0, Batch: 1087, Loss: nan\n",
      "Epoch: 0, Batch: 1088, Loss: nan\n",
      "Epoch: 0, Batch: 1089, Loss: nan\n",
      "Epoch: 0, Batch: 1090, Loss: nan\n",
      "Epoch: 0, Batch: 1091, Loss: nan\n",
      "Epoch: 0, Batch: 1092, Loss: nan\n",
      "Epoch: 0, Batch: 1093, Loss: nan\n",
      "Epoch: 0, Batch: 1094, Loss: nan\n",
      "Epoch: 0, Batch: 1095, Loss: nan\n",
      "Epoch: 0, Batch: 1096, Loss: nan\n",
      "Epoch: 0, Batch: 1097, Loss: nan\n",
      "Epoch: 0, Batch: 1098, Loss: nan\n",
      "Epoch: 0, Batch: 1099, Loss: nan\n",
      "Epoch: 0, Batch: 1100, Loss: nan\n",
      "Epoch: 0, Batch: 1101, Loss: nan\n",
      "Epoch: 0, Batch: 1102, Loss: nan\n",
      "Epoch: 0, Batch: 1103, Loss: nan\n",
      "Epoch: 0, Batch: 1104, Loss: nan\n",
      "Epoch: 0, Batch: 1105, Loss: nan\n",
      "Epoch: 0, Batch: 1106, Loss: nan\n",
      "Epoch: 0, Batch: 1107, Loss: nan\n",
      "Epoch: 0, Batch: 1108, Loss: nan\n",
      "Epoch: 0, Batch: 1109, Loss: nan\n",
      "Epoch: 0, Batch: 1110, Loss: nan\n",
      "Epoch: 0, Batch: 1111, Loss: nan\n",
      "Epoch: 0, Batch: 1112, Loss: nan\n",
      "Epoch: 0, Batch: 1113, Loss: nan\n",
      "Epoch: 0, Batch: 1114, Loss: nan\n",
      "Epoch: 0, Batch: 1115, Loss: nan\n",
      "Epoch: 0, Batch: 1116, Loss: nan\n",
      "Epoch: 0, Batch: 1117, Loss: nan\n",
      "Epoch: 0, Batch: 1118, Loss: nan\n",
      "Epoch: 0, Batch: 1119, Loss: nan\n",
      "Epoch: 0, Batch: 1120, Loss: nan\n",
      "Epoch: 0, Batch: 1121, Loss: nan\n",
      "Epoch: 0, Batch: 1122, Loss: nan\n",
      "Epoch: 0, Batch: 1123, Loss: nan\n",
      "Epoch: 0, Batch: 1124, Loss: nan\n",
      "Epoch: 0, Batch: 1125, Loss: nan\n",
      "Epoch: 0, Batch: 1126, Loss: nan\n",
      "Epoch: 0, Batch: 1127, Loss: nan\n",
      "Epoch: 0, Batch: 1128, Loss: nan\n",
      "Epoch: 0, Batch: 1129, Loss: nan\n",
      "Epoch: 0, Batch: 1130, Loss: nan\n",
      "Epoch: 0, Batch: 1131, Loss: nan\n",
      "Epoch: 0, Batch: 1132, Loss: nan\n",
      "Epoch: 0, Batch: 1133, Loss: nan\n",
      "Epoch: 0, Batch: 1134, Loss: nan\n",
      "Epoch: 0, Batch: 1135, Loss: nan\n",
      "Epoch: 0, Batch: 1136, Loss: nan\n",
      "Epoch: 0, Batch: 1137, Loss: nan\n",
      "Epoch: 0, Batch: 1138, Loss: nan\n",
      "Epoch: 0, Batch: 1139, Loss: nan\n",
      "Epoch: 0, Batch: 1140, Loss: nan\n",
      "Epoch: 0, Batch: 1141, Loss: nan\n",
      "Epoch: 0, Batch: 1142, Loss: nan\n",
      "Epoch: 0, Batch: 1143, Loss: nan\n",
      "Epoch: 0, Batch: 1144, Loss: nan\n",
      "Epoch: 0, Batch: 1145, Loss: nan\n",
      "Epoch: 0, Batch: 1146, Loss: nan\n",
      "Epoch: 0, Batch: 1147, Loss: nan\n",
      "Epoch: 0, Batch: 1148, Loss: nan\n",
      "Epoch: 0, Batch: 1149, Loss: nan\n",
      "Epoch: 0, Batch: 1150, Loss: nan\n",
      "Epoch: 0, Batch: 1151, Loss: nan\n",
      "Epoch: 0, Batch: 1152, Loss: nan\n",
      "Epoch: 0, Batch: 1153, Loss: nan\n",
      "Epoch: 0, Batch: 1154, Loss: nan\n",
      "Epoch: 0, Batch: 1155, Loss: nan\n",
      "Epoch: 0, Batch: 1156, Loss: nan\n",
      "Epoch: 0, Batch: 1157, Loss: nan\n",
      "Epoch: 0, Batch: 1158, Loss: nan\n",
      "Epoch: 0, Batch: 1159, Loss: nan\n",
      "Epoch: 0, Batch: 1160, Loss: nan\n",
      "Epoch: 0, Batch: 1161, Loss: nan\n",
      "Epoch: 0, Batch: 1162, Loss: nan\n",
      "Epoch: 0, Batch: 1163, Loss: nan\n",
      "Epoch: 0, Batch: 1164, Loss: nan\n",
      "Epoch: 0, Batch: 1165, Loss: nan\n",
      "Epoch: 0, Batch: 1166, Loss: nan\n",
      "Epoch: 0, Batch: 1167, Loss: nan\n",
      "Epoch: 0, Batch: 1168, Loss: nan\n",
      "Epoch: 0, Batch: 1169, Loss: nan\n",
      "Epoch: 0, Batch: 1170, Loss: nan\n",
      "Epoch: 0, Batch: 1171, Loss: nan\n",
      "Epoch: 0, Batch: 1172, Loss: nan\n",
      "Epoch: 0, Batch: 1173, Loss: nan\n",
      "Epoch: 0, Batch: 1174, Loss: nan\n",
      "Epoch: 0, Batch: 1175, Loss: nan\n",
      "Epoch: 0, Batch: 1176, Loss: nan\n",
      "Epoch: 0, Batch: 1177, Loss: nan\n",
      "Epoch: 0, Batch: 1178, Loss: nan\n",
      "Epoch: 0, Batch: 1179, Loss: nan\n",
      "Epoch: 0, Batch: 1180, Loss: nan\n",
      "Epoch: 0, Batch: 1181, Loss: nan\n",
      "Epoch: 0, Batch: 1182, Loss: nan\n",
      "Epoch: 0, Batch: 1183, Loss: nan\n",
      "Epoch: 0, Batch: 1184, Loss: nan\n",
      "Epoch: 0, Batch: 1185, Loss: nan\n",
      "Epoch: 0, Batch: 1186, Loss: nan\n",
      "Epoch: 0, Batch: 1187, Loss: nan\n",
      "Epoch: 0, Batch: 1188, Loss: nan\n",
      "Epoch: 0, Batch: 1189, Loss: nan\n",
      "Epoch: 0, Batch: 1190, Loss: nan\n",
      "Epoch: 0, Batch: 1191, Loss: nan\n",
      "Epoch: 0, Batch: 1192, Loss: nan\n",
      "Epoch: 0, Batch: 1193, Loss: nan\n",
      "Epoch: 0, Batch: 1194, Loss: nan\n",
      "Epoch: 0, Batch: 1195, Loss: nan\n",
      "Epoch: 0, Batch: 1196, Loss: nan\n",
      "Epoch: 0, Batch: 1197, Loss: nan\n",
      "Epoch: 0, Batch: 1198, Loss: nan\n",
      "Epoch: 0, Batch: 1199, Loss: nan\n",
      "Epoch: 0, Batch: 1200, Loss: nan\n",
      "Epoch: 0, Batch: 1201, Loss: nan\n",
      "Epoch: 0, Batch: 1202, Loss: nan\n",
      "Epoch: 0, Batch: 1203, Loss: nan\n",
      "Epoch: 0, Batch: 1204, Loss: nan\n",
      "Epoch: 0, Batch: 1205, Loss: nan\n",
      "Epoch: 0, Batch: 1206, Loss: nan\n",
      "Epoch: 0, Batch: 1207, Loss: nan\n",
      "Epoch: 0, Batch: 1208, Loss: nan\n",
      "Epoch: 0, Batch: 1209, Loss: nan\n",
      "Epoch: 0, Batch: 1210, Loss: nan\n",
      "Epoch: 0, Batch: 1211, Loss: nan\n",
      "Epoch: 0, Batch: 1212, Loss: nan\n",
      "Epoch: 0, Batch: 1213, Loss: nan\n",
      "Epoch: 0, Batch: 1214, Loss: nan\n",
      "Epoch: 0, Batch: 1215, Loss: nan\n",
      "Epoch: 0, Batch: 1216, Loss: nan\n",
      "Epoch: 0, Batch: 1217, Loss: nan\n",
      "Epoch: 0, Batch: 1218, Loss: nan\n",
      "Epoch: 0, Batch: 1219, Loss: nan\n",
      "Epoch: 0, Batch: 1220, Loss: nan\n",
      "Epoch: 0, Batch: 1221, Loss: nan\n",
      "Epoch: 0, Batch: 1222, Loss: nan\n",
      "Epoch: 0, Batch: 1223, Loss: nan\n",
      "Epoch: 0, Batch: 1224, Loss: nan\n",
      "Epoch: 0, Batch: 1225, Loss: nan\n",
      "Epoch: 0, Batch: 1226, Loss: nan\n",
      "Epoch: 0, Batch: 1227, Loss: nan\n",
      "Epoch: 0, Batch: 1228, Loss: nan\n",
      "Epoch: 0, Batch: 1229, Loss: nan\n",
      "Epoch: 0, Batch: 1230, Loss: nan\n",
      "Epoch: 0, Batch: 1231, Loss: nan\n",
      "Epoch: 0, Batch: 1232, Loss: nan\n",
      "Epoch: 0, Batch: 1233, Loss: nan\n",
      "Epoch: 0, Batch: 1234, Loss: nan\n",
      "Epoch: 0, Batch: 1235, Loss: nan\n",
      "Epoch: 0, Batch: 1236, Loss: nan\n",
      "Epoch: 0, Batch: 1237, Loss: nan\n",
      "Epoch: 0, Batch: 1238, Loss: nan\n",
      "Epoch: 0, Batch: 1239, Loss: nan\n",
      "Epoch: 0, Batch: 1240, Loss: nan\n",
      "Epoch: 0, Batch: 1241, Loss: nan\n",
      "Epoch: 0, Batch: 1242, Loss: nan\n",
      "Epoch: 0, Batch: 1243, Loss: nan\n",
      "Epoch: 0, Batch: 1244, Loss: nan\n",
      "Epoch: 0, Batch: 1245, Loss: nan\n",
      "Epoch: 0, Batch: 1246, Loss: nan\n",
      "Epoch: 0, Batch: 1247, Loss: nan\n",
      "Epoch: 0, Batch: 1248, Loss: nan\n",
      "Epoch: 0, Batch: 1249, Loss: nan\n",
      "Epoch: 0, Batch: 1250, Loss: nan\n",
      "Epoch: 0, Batch: 1251, Loss: nan\n",
      "Epoch: 0, Batch: 1252, Loss: nan\n",
      "Epoch: 0, Batch: 1253, Loss: nan\n",
      "Epoch: 0, Batch: 1254, Loss: nan\n",
      "Epoch: 0, Batch: 1255, Loss: nan\n",
      "Epoch: 0, Batch: 1256, Loss: nan\n",
      "Epoch: 0, Batch: 1257, Loss: nan\n",
      "Epoch: 0, Batch: 1258, Loss: nan\n",
      "Epoch: 0, Batch: 1259, Loss: nan\n",
      "Epoch: 0, Batch: 1260, Loss: nan\n",
      "Epoch: 0, Batch: 1261, Loss: nan\n",
      "Epoch: 0, Batch: 1262, Loss: nan\n",
      "Epoch: 0, Batch: 1263, Loss: nan\n",
      "Epoch: 0, Batch: 1264, Loss: nan\n",
      "Epoch: 0, Batch: 1265, Loss: nan\n",
      "Epoch: 0, Batch: 1266, Loss: nan\n",
      "Epoch: 0, Batch: 1267, Loss: nan\n",
      "Epoch: 0, Batch: 1268, Loss: nan\n",
      "Epoch: 0, Batch: 1269, Loss: nan\n",
      "Epoch: 0, Batch: 1270, Loss: nan\n",
      "Epoch: 0, Batch: 1271, Loss: nan\n",
      "Epoch: 0, Batch: 1272, Loss: nan\n",
      "Epoch: 0, Batch: 1273, Loss: nan\n",
      "Epoch: 0, Batch: 1274, Loss: nan\n",
      "Epoch: 0, Batch: 1275, Loss: nan\n",
      "Epoch: 0, Batch: 1276, Loss: nan\n",
      "Epoch: 0, Batch: 1277, Loss: nan\n",
      "Epoch: 0, Batch: 1278, Loss: nan\n",
      "Epoch: 0, Batch: 1279, Loss: nan\n",
      "Epoch: 0, Batch: 1280, Loss: nan\n",
      "Epoch: 0, Batch: 1281, Loss: nan\n",
      "Epoch: 0, Batch: 1282, Loss: nan\n",
      "Epoch: 0, Batch: 1283, Loss: nan\n",
      "Epoch: 0, Batch: 1284, Loss: nan\n",
      "Epoch: 0, Batch: 1285, Loss: nan\n",
      "Epoch: 0, Batch: 1286, Loss: nan\n",
      "Epoch: 0, Batch: 1287, Loss: nan\n",
      "Epoch: 0, Batch: 1288, Loss: nan\n",
      "Epoch: 0, Batch: 1289, Loss: nan\n",
      "Epoch: 0, Batch: 1290, Loss: nan\n",
      "Epoch: 0, Batch: 1291, Loss: nan\n",
      "Epoch: 0, Batch: 1292, Loss: nan\n",
      "Epoch: 0, Batch: 1293, Loss: nan\n",
      "Epoch: 0, Batch: 1294, Loss: nan\n",
      "Epoch: 0, Batch: 1295, Loss: nan\n",
      "Epoch: 0, Batch: 1296, Loss: nan\n",
      "Epoch: 0, Batch: 1297, Loss: nan\n",
      "Epoch: 0, Batch: 1298, Loss: nan\n",
      "Epoch: 0, Batch: 1299, Loss: nan\n",
      "Epoch: 0, Batch: 1300, Loss: nan\n",
      "Epoch: 0, Batch: 1301, Loss: nan\n",
      "Epoch: 0, Batch: 1302, Loss: nan\n",
      "Epoch: 0, Batch: 1303, Loss: nan\n",
      "Epoch: 0, Batch: 1304, Loss: nan\n",
      "Epoch: 0, Batch: 1305, Loss: nan\n",
      "Epoch: 0, Batch: 1306, Loss: nan\n",
      "Epoch: 0, Batch: 1307, Loss: nan\n",
      "Epoch: 0, Batch: 1308, Loss: nan\n",
      "Epoch: 0, Batch: 1309, Loss: nan\n",
      "Epoch: 0, Batch: 1310, Loss: nan\n",
      "Epoch: 0, Batch: 1311, Loss: nan\n",
      "Epoch: 0, Batch: 1312, Loss: nan\n",
      "Epoch: 0, Batch: 1313, Loss: nan\n",
      "Epoch: 0, Batch: 1314, Loss: nan\n",
      "Epoch: 0, Batch: 1315, Loss: nan\n",
      "Epoch: 0, Batch: 1316, Loss: nan\n",
      "Epoch: 0, Batch: 1317, Loss: nan\n",
      "Epoch: 0, Batch: 1318, Loss: nan\n",
      "Epoch: 0, Batch: 1319, Loss: nan\n",
      "Epoch: 0, Batch: 1320, Loss: nan\n",
      "Epoch: 0, Batch: 1321, Loss: nan\n",
      "Epoch: 0, Batch: 1322, Loss: nan\n",
      "Epoch: 0, Batch: 1323, Loss: nan\n",
      "Epoch: 0, Batch: 1324, Loss: nan\n",
      "Epoch: 0, Batch: 1325, Loss: nan\n",
      "Epoch: 0, Batch: 1326, Loss: nan\n",
      "Epoch: 0, Batch: 1327, Loss: nan\n",
      "Epoch: 0, Batch: 1328, Loss: nan\n",
      "Epoch: 0, Batch: 1329, Loss: nan\n",
      "Epoch: 0, Batch: 1330, Loss: nan\n",
      "Epoch: 0, Batch: 1331, Loss: nan\n",
      "Epoch: 0, Batch: 1332, Loss: nan\n",
      "Epoch: 0, Batch: 1333, Loss: nan\n",
      "Epoch: 0, Batch: 1334, Loss: nan\n",
      "Epoch: 0, Batch: 1335, Loss: nan\n",
      "Epoch: 0, Batch: 1336, Loss: nan\n",
      "Epoch: 0, Batch: 1337, Loss: nan\n",
      "Epoch: 0, Batch: 1338, Loss: nan\n",
      "Epoch: 0, Batch: 1339, Loss: nan\n",
      "Epoch: 0, Batch: 1340, Loss: nan\n",
      "Epoch: 0, Batch: 1341, Loss: nan\n",
      "Epoch: 0, Batch: 1342, Loss: nan\n",
      "Epoch: 0, Batch: 1343, Loss: nan\n",
      "Epoch: 0, Batch: 1344, Loss: nan\n",
      "Epoch: 0, Batch: 1345, Loss: nan\n",
      "Epoch: 0, Batch: 1346, Loss: nan\n",
      "Epoch: 0, Batch: 1347, Loss: nan\n",
      "Epoch: 0, Batch: 1348, Loss: nan\n",
      "Epoch: 0, Batch: 1349, Loss: nan\n",
      "Epoch: 0, Batch: 1350, Loss: nan\n",
      "Epoch: 0, Batch: 1351, Loss: nan\n",
      "Epoch: 0, Batch: 1352, Loss: nan\n",
      "Epoch: 0, Batch: 1353, Loss: nan\n",
      "Epoch: 0, Batch: 1354, Loss: nan\n",
      "Epoch: 0, Batch: 1355, Loss: nan\n",
      "Epoch: 0, Batch: 1356, Loss: nan\n",
      "Epoch: 0, Batch: 1357, Loss: nan\n",
      "Epoch: 0, Batch: 1358, Loss: nan\n",
      "Epoch: 0, Batch: 1359, Loss: nan\n",
      "Epoch: 0, Batch: 1360, Loss: nan\n",
      "Epoch: 0, Batch: 1361, Loss: nan\n",
      "Epoch: 0, Batch: 1362, Loss: nan\n",
      "Epoch: 0, Batch: 1363, Loss: nan\n",
      "Epoch: 0, Batch: 1364, Loss: nan\n",
      "Epoch: 0, Batch: 1365, Loss: nan\n",
      "Epoch: 0, Batch: 1366, Loss: nan\n",
      "Epoch: 0, Batch: 1367, Loss: nan\n",
      "Epoch: 0, Batch: 1368, Loss: nan\n",
      "Epoch: 0, Batch: 1369, Loss: nan\n",
      "Epoch: 0, Batch: 1370, Loss: nan\n",
      "Epoch: 0, Batch: 1371, Loss: nan\n",
      "Epoch: 0, Batch: 1372, Loss: nan\n",
      "Epoch: 0, Batch: 1373, Loss: nan\n",
      "Epoch: 0, Batch: 1374, Loss: nan\n",
      "Epoch: 0, Batch: 1375, Loss: nan\n",
      "Epoch: 0, Batch: 1376, Loss: nan\n",
      "Epoch: 0, Batch: 1377, Loss: nan\n",
      "Epoch: 0, Batch: 1378, Loss: nan\n",
      "Epoch: 0, Batch: 1379, Loss: nan\n",
      "Epoch: 0, Batch: 1380, Loss: nan\n",
      "Epoch: 0, Batch: 1381, Loss: nan\n",
      "Epoch: 0, Batch: 1382, Loss: nan\n",
      "Epoch: 0, Batch: 1383, Loss: nan\n",
      "Epoch: 0, Batch: 1384, Loss: nan\n",
      "Epoch: 0, Batch: 1385, Loss: nan\n",
      "Epoch: 0, Batch: 1386, Loss: nan\n",
      "Epoch: 0, Batch: 1387, Loss: nan\n",
      "Epoch: 0, Batch: 1388, Loss: nan\n",
      "Epoch: 0, Batch: 1389, Loss: nan\n",
      "Epoch: 0, Batch: 1390, Loss: nan\n",
      "Epoch: 0, Batch: 1391, Loss: nan\n",
      "Epoch: 0, Batch: 1392, Loss: nan\n",
      "Epoch: 0, Batch: 1393, Loss: nan\n",
      "Epoch: 0, Batch: 1394, Loss: nan\n",
      "Epoch: 0, Batch: 1395, Loss: nan\n",
      "Epoch: 0, Batch: 1396, Loss: nan\n",
      "Epoch: 0, Batch: 1397, Loss: nan\n",
      "Epoch: 0, Batch: 1398, Loss: nan\n",
      "Epoch: 0, Batch: 1399, Loss: nan\n",
      "Epoch: 0, Batch: 1400, Loss: nan\n",
      "Epoch: 0, Batch: 1401, Loss: nan\n",
      "Epoch: 0, Batch: 1402, Loss: nan\n",
      "Epoch: 0, Batch: 1403, Loss: nan\n",
      "Epoch: 0, Batch: 1404, Loss: nan\n",
      "Epoch: 0, Batch: 1405, Loss: nan\n",
      "Epoch: 0, Batch: 1406, Loss: nan\n",
      "Epoch: 0, Batch: 1407, Loss: nan\n",
      "Epoch: 0, Batch: 1408, Loss: nan\n",
      "Epoch: 0, Batch: 1409, Loss: nan\n",
      "Epoch: 0, Batch: 1410, Loss: nan\n",
      "Epoch: 0, Batch: 1411, Loss: nan\n",
      "Epoch: 0, Batch: 1412, Loss: nan\n",
      "Epoch: 0, Batch: 1413, Loss: nan\n",
      "Epoch: 0, Batch: 1414, Loss: nan\n",
      "Epoch: 0, Batch: 1415, Loss: nan\n",
      "Epoch: 0, Batch: 1416, Loss: nan\n",
      "Epoch: 0, Batch: 1417, Loss: nan\n",
      "Epoch: 0, Batch: 1418, Loss: nan\n",
      "Epoch: 0, Batch: 1419, Loss: nan\n",
      "Epoch: 0, Batch: 1420, Loss: nan\n",
      "Epoch: 0, Batch: 1421, Loss: nan\n",
      "Epoch: 0, Batch: 1422, Loss: nan\n",
      "Epoch: 0, Batch: 1423, Loss: nan\n",
      "Epoch: 0, Batch: 1424, Loss: nan\n",
      "Epoch: 0, Batch: 1425, Loss: nan\n",
      "Epoch: 0, Batch: 1426, Loss: nan\n",
      "Epoch: 0, Batch: 1427, Loss: nan\n",
      "Epoch: 0, Batch: 1428, Loss: nan\n",
      "Epoch: 0, Batch: 1429, Loss: nan\n",
      "Epoch: 0, Batch: 1430, Loss: nan\n",
      "Epoch: 0, Batch: 1431, Loss: nan\n",
      "Epoch: 0, Batch: 1432, Loss: nan\n",
      "Epoch: 0, Batch: 1433, Loss: nan\n",
      "Epoch: 0, Batch: 1434, Loss: nan\n",
      "Epoch: 0, Batch: 1435, Loss: nan\n",
      "Epoch: 0, Batch: 1436, Loss: nan\n",
      "Epoch: 0, Batch: 1437, Loss: nan\n",
      "Epoch: 0, Batch: 1438, Loss: nan\n",
      "Epoch: 0, Batch: 1439, Loss: nan\n",
      "Epoch: 0, Batch: 1440, Loss: nan\n",
      "Epoch: 0, Batch: 1441, Loss: nan\n",
      "Epoch: 0, Batch: 1442, Loss: nan\n",
      "Epoch: 0, Batch: 1443, Loss: nan\n",
      "Epoch: 0, Batch: 1444, Loss: nan\n",
      "Epoch: 0, Batch: 1445, Loss: nan\n",
      "Epoch: 0, Batch: 1446, Loss: nan\n",
      "Epoch: 0, Batch: 1447, Loss: nan\n",
      "Epoch: 0, Batch: 1448, Loss: nan\n",
      "Epoch: 0, Batch: 1449, Loss: nan\n",
      "Epoch: 0, Batch: 1450, Loss: nan\n",
      "Epoch: 0, Batch: 1451, Loss: nan\n",
      "Epoch: 0, Batch: 1452, Loss: nan\n",
      "Epoch: 0, Batch: 1453, Loss: nan\n",
      "Epoch: 0, Batch: 1454, Loss: nan\n",
      "Epoch: 0, Batch: 1455, Loss: nan\n",
      "Epoch: 0, Batch: 1456, Loss: nan\n",
      "Epoch: 0, Batch: 1457, Loss: nan\n",
      "Epoch: 0, Batch: 1458, Loss: nan\n",
      "Epoch: 0, Batch: 1459, Loss: nan\n",
      "Epoch: 0, Batch: 1460, Loss: nan\n",
      "Epoch: 0, Batch: 1461, Loss: nan\n",
      "Epoch: 0, Batch: 1462, Loss: nan\n",
      "Epoch: 0, Batch: 1463, Loss: nan\n",
      "Epoch: 0, Batch: 1464, Loss: nan\n",
      "Epoch: 0, Batch: 1465, Loss: nan\n",
      "Epoch: 0, Batch: 1466, Loss: nan\n",
      "Epoch: 0, Batch: 1467, Loss: nan\n",
      "Epoch: 0, Batch: 1468, Loss: nan\n",
      "Epoch: 0, Batch: 1469, Loss: nan\n",
      "Epoch: 0, Batch: 1470, Loss: nan\n",
      "Epoch: 0, Batch: 1471, Loss: nan\n",
      "Epoch: 0, Batch: 1472, Loss: nan\n",
      "Epoch: 0, Batch: 1473, Loss: nan\n",
      "Epoch: 0, Batch: 1474, Loss: nan\n",
      "Epoch: 0, Batch: 1475, Loss: nan\n",
      "Epoch: 0, Batch: 1476, Loss: nan\n",
      "Epoch: 0, Batch: 1477, Loss: nan\n",
      "Epoch: 0, Batch: 1478, Loss: nan\n",
      "Epoch: 0, Batch: 1479, Loss: nan\n",
      "Epoch: 0, Batch: 1480, Loss: nan\n",
      "Epoch: 0, Batch: 1481, Loss: nan\n",
      "Epoch: 0, Batch: 1482, Loss: nan\n",
      "Epoch: 0, Batch: 1483, Loss: nan\n",
      "Epoch: 0, Batch: 1484, Loss: nan\n",
      "Epoch: 0, Batch: 1485, Loss: nan\n",
      "Epoch: 0, Batch: 1486, Loss: nan\n",
      "Epoch: 0, Batch: 1487, Loss: nan\n",
      "Epoch: 0, Batch: 1488, Loss: nan\n",
      "Epoch: 0, Batch: 1489, Loss: nan\n",
      "Epoch: 0, Batch: 1490, Loss: nan\n",
      "Epoch: 0, Batch: 1491, Loss: nan\n",
      "Epoch: 0, Batch: 1492, Loss: nan\n",
      "Epoch: 0, Batch: 1493, Loss: nan\n",
      "Epoch: 0, Batch: 1494, Loss: nan\n",
      "Epoch: 0, Batch: 1495, Loss: nan\n",
      "Epoch: 0, Batch: 1496, Loss: nan\n",
      "Epoch: 0, Batch: 1497, Loss: nan\n",
      "Epoch: 0, Batch: 1498, Loss: nan\n",
      "Epoch: 0, Batch: 1499, Loss: nan\n",
      "Epoch: 0, Batch: 1500, Loss: nan\n",
      "Epoch: 0, Batch: 1501, Loss: nan\n",
      "Epoch: 0, Batch: 1502, Loss: nan\n",
      "Epoch: 0, Batch: 1503, Loss: nan\n",
      "Epoch: 0, Batch: 1504, Loss: nan\n",
      "Epoch: 0, Batch: 1505, Loss: nan\n",
      "Epoch: 0, Batch: 1506, Loss: nan\n",
      "Epoch: 0, Batch: 1507, Loss: nan\n",
      "Epoch: 0, Batch: 1508, Loss: nan\n",
      "Epoch: 0, Batch: 1509, Loss: nan\n",
      "Epoch: 0, Batch: 1510, Loss: nan\n",
      "Epoch: 0, Batch: 1511, Loss: nan\n",
      "Epoch: 0, Batch: 1512, Loss: nan\n",
      "Epoch: 0, Batch: 1513, Loss: nan\n",
      "Epoch: 0, Batch: 1514, Loss: nan\n",
      "Epoch: 0, Batch: 1515, Loss: nan\n",
      "Epoch: 0, Batch: 1516, Loss: nan\n",
      "Epoch: 0, Batch: 1517, Loss: nan\n",
      "Epoch: 0, Batch: 1518, Loss: nan\n",
      "Epoch: 0, Batch: 1519, Loss: nan\n",
      "Epoch: 0, Batch: 1520, Loss: nan\n",
      "Epoch: 0, Batch: 1521, Loss: nan\n",
      "Epoch: 0, Batch: 1522, Loss: nan\n",
      "Epoch: 0, Batch: 1523, Loss: nan\n",
      "Epoch: 0, Batch: 1524, Loss: nan\n",
      "Epoch: 0, Batch: 1525, Loss: nan\n",
      "Epoch: 0, Batch: 1526, Loss: nan\n",
      "Epoch: 0, Batch: 1527, Loss: nan\n",
      "Epoch: 0, Batch: 1528, Loss: nan\n",
      "Epoch: 0, Batch: 1529, Loss: nan\n",
      "Epoch: 0, Batch: 1530, Loss: nan\n",
      "Epoch: 0, Batch: 1531, Loss: nan\n",
      "Epoch: 0, Batch: 1532, Loss: nan\n",
      "Epoch: 0, Batch: 1533, Loss: nan\n",
      "Epoch: 0, Batch: 1534, Loss: nan\n",
      "Epoch: 0, Batch: 1535, Loss: nan\n",
      "Epoch: 0, Batch: 1536, Loss: nan\n",
      "Epoch: 0, Batch: 1537, Loss: nan\n",
      "Epoch: 0, Batch: 1538, Loss: nan\n",
      "Epoch: 0, Batch: 1539, Loss: nan\n",
      "Epoch: 0, Batch: 1540, Loss: nan\n",
      "Epoch: 0, Batch: 1541, Loss: nan\n",
      "Epoch: 0, Batch: 1542, Loss: nan\n",
      "Epoch: 0, Batch: 1543, Loss: nan\n",
      "Epoch: 0, Batch: 1544, Loss: nan\n",
      "Epoch: 0, Batch: 1545, Loss: nan\n",
      "Epoch: 0, Batch: 1546, Loss: nan\n",
      "Epoch: 0, Batch: 1547, Loss: nan\n",
      "Epoch: 0, Batch: 1548, Loss: nan\n",
      "Epoch: 0, Batch: 1549, Loss: nan\n",
      "Epoch: 0, Batch: 1550, Loss: nan\n",
      "Epoch: 0, Batch: 1551, Loss: nan\n",
      "Epoch: 0, Batch: 1552, Loss: nan\n",
      "Epoch: 0, Batch: 1553, Loss: nan\n",
      "Epoch: 0, Batch: 1554, Loss: nan\n",
      "Epoch: 0, Batch: 1555, Loss: nan\n",
      "Epoch: 0, Batch: 1556, Loss: nan\n",
      "Epoch: 0, Batch: 1557, Loss: nan\n",
      "Epoch: 0, Batch: 1558, Loss: nan\n",
      "Epoch: 0, Batch: 1559, Loss: nan\n",
      "Epoch: 0, Batch: 1560, Loss: nan\n",
      "Epoch: 0, Batch: 1561, Loss: nan\n",
      "Epoch: 0, Batch: 1562, Loss: nan\n",
      "Epoch: 0, Batch: 1563, Loss: nan\n",
      "Epoch: 0, Batch: 1564, Loss: nan\n",
      "Epoch: 0, Batch: 1565, Loss: nan\n",
      "Epoch: 0, Batch: 1566, Loss: nan\n",
      "Epoch: 0, Batch: 1567, Loss: nan\n",
      "Epoch: 0, Batch: 1568, Loss: nan\n",
      "Epoch: 0, Batch: 1569, Loss: nan\n",
      "Epoch: 0, Batch: 1570, Loss: nan\n",
      "Epoch: 0, Batch: 1571, Loss: nan\n",
      "Epoch: 0, Batch: 1572, Loss: nan\n",
      "Epoch: 0, Batch: 1573, Loss: nan\n",
      "Epoch: 0, Batch: 1574, Loss: nan\n",
      "Epoch: 0, Batch: 1575, Loss: nan\n",
      "Epoch: 0, Batch: 1576, Loss: nan\n",
      "Epoch: 0, Batch: 1577, Loss: nan\n",
      "Epoch: 0, Batch: 1578, Loss: nan\n",
      "Epoch: 0, Batch: 1579, Loss: nan\n",
      "Epoch: 0, Batch: 1580, Loss: nan\n",
      "Epoch: 0, Batch: 1581, Loss: nan\n",
      "Epoch: 0, Batch: 1582, Loss: nan\n",
      "Epoch: 0, Batch: 1583, Loss: nan\n",
      "Epoch: 0, Batch: 1584, Loss: nan\n",
      "Epoch: 0, Batch: 1585, Loss: nan\n",
      "Epoch: 0, Batch: 1586, Loss: nan\n",
      "Epoch: 0, Batch: 1587, Loss: nan\n",
      "Epoch: 0, Batch: 1588, Loss: nan\n",
      "Epoch: 0, Batch: 1589, Loss: nan\n",
      "Epoch: 0, Batch: 1590, Loss: nan\n",
      "Epoch: 0, Batch: 1591, Loss: nan\n",
      "Epoch: 0, Batch: 1592, Loss: nan\n",
      "Epoch: 0, Batch: 1593, Loss: nan\n",
      "Epoch: 0, Batch: 1594, Loss: nan\n",
      "Epoch: 0, Batch: 1595, Loss: nan\n",
      "Epoch: 0, Batch: 1596, Loss: nan\n",
      "Epoch: 0, Batch: 1597, Loss: nan\n",
      "Epoch: 0, Batch: 1598, Loss: nan\n",
      "Epoch: 0, Batch: 1599, Loss: nan\n",
      "Epoch: 0, Batch: 1600, Loss: nan\n",
      "Epoch: 0, Batch: 1601, Loss: nan\n",
      "Epoch: 0, Batch: 1602, Loss: nan\n",
      "Epoch: 0, Batch: 1603, Loss: nan\n",
      "Epoch: 0, Batch: 1604, Loss: nan\n",
      "Epoch: 0, Batch: 1605, Loss: nan\n",
      "Epoch: 0, Batch: 1606, Loss: nan\n",
      "Epoch: 0, Batch: 1607, Loss: nan\n",
      "Epoch: 0, Batch: 1608, Loss: nan\n",
      "Epoch: 0, Batch: 1609, Loss: nan\n",
      "Epoch: 0, Batch: 1610, Loss: nan\n",
      "Epoch: 0, Batch: 1611, Loss: nan\n",
      "Epoch: 0, Batch: 1612, Loss: nan\n",
      "Epoch: 0, Batch: 1613, Loss: nan\n",
      "Epoch: 0, Batch: 1614, Loss: nan\n",
      "Epoch: 0, Batch: 1615, Loss: nan\n",
      "Epoch: 0, Batch: 1616, Loss: nan\n",
      "Epoch: 0, Batch: 1617, Loss: nan\n",
      "Epoch: 0, Batch: 1618, Loss: nan\n",
      "Epoch: 0, Batch: 1619, Loss: nan\n",
      "Epoch: 0, Batch: 1620, Loss: nan\n",
      "Epoch: 0, Batch: 1621, Loss: nan\n",
      "Epoch: 0, Batch: 1622, Loss: nan\n",
      "Epoch: 0, Batch: 1623, Loss: nan\n",
      "Epoch: 0, Batch: 1624, Loss: nan\n",
      "Epoch: 0, Batch: 1625, Loss: nan\n",
      "Epoch: 0, Batch: 1626, Loss: nan\n",
      "Epoch: 0, Batch: 1627, Loss: nan\n",
      "Epoch: 0, Batch: 1628, Loss: nan\n",
      "Epoch: 0, Batch: 1629, Loss: nan\n",
      "Epoch: 0, Batch: 1630, Loss: nan\n",
      "Epoch: 0, Batch: 1631, Loss: nan\n",
      "Epoch: 0, Batch: 1632, Loss: nan\n",
      "Epoch: 0, Batch: 1633, Loss: nan\n",
      "Epoch: 0, Batch: 1634, Loss: nan\n",
      "Epoch: 0, Batch: 1635, Loss: nan\n",
      "Epoch: 0, Batch: 1636, Loss: nan\n",
      "Epoch: 0, Batch: 1637, Loss: nan\n",
      "Epoch: 0, Batch: 1638, Loss: nan\n",
      "Epoch: 0, Batch: 1639, Loss: nan\n",
      "Epoch: 0, Batch: 1640, Loss: nan\n",
      "Epoch: 0, Batch: 1641, Loss: nan\n",
      "Epoch: 0, Batch: 1642, Loss: nan\n",
      "Epoch: 0, Batch: 1643, Loss: nan\n",
      "Epoch: 0, Batch: 1644, Loss: nan\n",
      "Epoch: 0, Batch: 1645, Loss: nan\n",
      "Epoch: 0, Batch: 1646, Loss: nan\n",
      "Epoch: 0, Batch: 1647, Loss: nan\n",
      "Epoch: 0, Batch: 1648, Loss: nan\n",
      "Epoch: 0, Batch: 1649, Loss: nan\n",
      "Epoch: 0, Batch: 1650, Loss: nan\n",
      "Epoch: 0, Batch: 1651, Loss: nan\n",
      "Epoch: 0, Batch: 1652, Loss: nan\n",
      "Epoch: 0, Batch: 1653, Loss: nan\n",
      "Epoch: 0, Batch: 1654, Loss: nan\n",
      "Epoch: 0, Batch: 1655, Loss: nan\n",
      "Epoch: 0, Batch: 1656, Loss: nan\n",
      "Epoch: 0, Batch: 1657, Loss: nan\n",
      "Epoch: 0, Batch: 1658, Loss: nan\n",
      "Epoch: 0, Batch: 1659, Loss: nan\n",
      "Epoch: 0, Batch: 1660, Loss: nan\n",
      "Epoch: 0, Batch: 1661, Loss: nan\n",
      "Epoch: 0, Batch: 1662, Loss: nan\n",
      "Epoch: 0, Batch: 1663, Loss: nan\n",
      "Epoch: 0, Batch: 1664, Loss: nan\n",
      "Epoch: 0, Batch: 1665, Loss: nan\n",
      "Epoch: 0, Batch: 1666, Loss: nan\n",
      "Epoch: 0, Batch: 1667, Loss: nan\n",
      "Epoch: 0, Batch: 1668, Loss: nan\n",
      "Epoch: 0, Batch: 1669, Loss: nan\n",
      "Epoch: 0, Batch: 1670, Loss: nan\n",
      "Epoch: 0, Batch: 1671, Loss: nan\n",
      "Epoch: 0, Batch: 1672, Loss: nan\n",
      "Epoch: 0, Batch: 1673, Loss: nan\n",
      "Epoch: 0, Batch: 1674, Loss: nan\n",
      "Epoch: 0, Batch: 1675, Loss: nan\n",
      "Epoch: 0, Batch: 1676, Loss: nan\n",
      "Epoch: 0, Batch: 1677, Loss: nan\n",
      "Epoch: 0, Batch: 1678, Loss: nan\n",
      "Epoch: 0, Batch: 1679, Loss: nan\n",
      "Epoch: 0, Batch: 1680, Loss: nan\n",
      "Epoch: 0, Batch: 1681, Loss: nan\n",
      "Epoch: 0, Batch: 1682, Loss: nan\n",
      "Epoch: 0, Batch: 1683, Loss: nan\n",
      "Epoch: 0, Batch: 1684, Loss: nan\n",
      "Epoch: 0, Batch: 1685, Loss: nan\n",
      "Epoch: 0, Batch: 1686, Loss: nan\n",
      "Epoch: 0, Batch: 1687, Loss: nan\n",
      "Epoch: 0, Batch: 1688, Loss: nan\n",
      "Epoch: 0, Batch: 1689, Loss: nan\n",
      "Epoch: 0, Batch: 1690, Loss: nan\n",
      "Epoch: 0, Batch: 1691, Loss: nan\n",
      "Epoch: 0, Batch: 1692, Loss: nan\n",
      "Epoch: 0, Batch: 1693, Loss: nan\n",
      "Epoch: 0, Batch: 1694, Loss: nan\n",
      "Epoch: 0, Batch: 1695, Loss: nan\n",
      "Epoch: 0, Batch: 1696, Loss: nan\n",
      "Epoch: 0, Batch: 1697, Loss: nan\n",
      "Epoch: 0, Batch: 1698, Loss: nan\n",
      "Epoch: 0, Batch: 1699, Loss: nan\n",
      "Epoch: 0, Batch: 1700, Loss: nan\n",
      "Epoch: 0, Batch: 1701, Loss: nan\n",
      "Epoch: 0, Batch: 1702, Loss: nan\n",
      "Epoch: 0, Batch: 1703, Loss: nan\n",
      "Epoch: 0, Batch: 1704, Loss: nan\n",
      "Epoch: 0, Batch: 1705, Loss: nan\n",
      "Epoch: 0, Batch: 1706, Loss: nan\n",
      "Epoch: 0, Batch: 1707, Loss: nan\n",
      "Epoch: 0, Batch: 1708, Loss: nan\n",
      "Epoch: 0, Batch: 1709, Loss: nan\n",
      "Epoch: 0, Batch: 1710, Loss: nan\n",
      "Epoch: 0, Batch: 1711, Loss: nan\n",
      "Epoch: 0, Batch: 1712, Loss: nan\n",
      "Epoch: 0, Batch: 1713, Loss: nan\n",
      "Epoch: 0, Batch: 1714, Loss: nan\n",
      "Epoch: 0, Batch: 1715, Loss: nan\n",
      "Epoch: 0, Batch: 1716, Loss: nan\n",
      "Epoch: 0, Batch: 1717, Loss: nan\n",
      "Epoch: 0, Batch: 1718, Loss: nan\n",
      "Epoch: 0, Batch: 1719, Loss: nan\n",
      "Epoch: 0, Batch: 1720, Loss: nan\n",
      "Epoch: 0, Batch: 1721, Loss: nan\n",
      "Epoch: 0, Batch: 1722, Loss: nan\n",
      "Epoch: 0, Batch: 1723, Loss: nan\n",
      "Epoch: 0, Batch: 1724, Loss: nan\n",
      "Epoch: 0, Batch: 1725, Loss: nan\n",
      "Epoch: 0, Batch: 1726, Loss: nan\n",
      "Epoch: 0, Batch: 1727, Loss: nan\n",
      "Epoch: 0, Batch: 1728, Loss: nan\n",
      "Epoch: 0, Batch: 1729, Loss: nan\n",
      "Epoch: 0, Batch: 1730, Loss: nan\n",
      "Epoch: 0, Batch: 1731, Loss: nan\n",
      "Epoch: 0, Batch: 1732, Loss: nan\n",
      "Epoch: 0, Batch: 1733, Loss: nan\n",
      "Epoch: 0, Batch: 1734, Loss: nan\n",
      "Epoch: 0, Batch: 1735, Loss: nan\n",
      "Epoch: 0, Batch: 1736, Loss: nan\n",
      "Epoch: 0, Batch: 1737, Loss: nan\n",
      "Epoch: 0, Batch: 1738, Loss: nan\n",
      "Epoch: 0, Batch: 1739, Loss: nan\n",
      "Epoch: 0, Batch: 1740, Loss: nan\n",
      "Epoch: 0, Batch: 1741, Loss: nan\n",
      "Epoch: 0, Batch: 1742, Loss: nan\n",
      "Epoch: 0, Batch: 1743, Loss: nan\n",
      "Epoch: 0, Batch: 1744, Loss: nan\n",
      "Epoch: 0, Batch: 1745, Loss: nan\n",
      "Epoch: 0, Batch: 1746, Loss: nan\n",
      "Epoch: 0, Batch: 1747, Loss: nan\n",
      "Epoch: 0, Batch: 1748, Loss: nan\n",
      "Epoch: 0, Batch: 1749, Loss: nan\n",
      "Epoch: 0, Batch: 1750, Loss: nan\n",
      "Epoch: 0, Batch: 1751, Loss: nan\n",
      "Epoch: 0, Batch: 1752, Loss: nan\n",
      "Epoch: 0, Batch: 1753, Loss: nan\n",
      "Epoch: 0, Batch: 1754, Loss: nan\n",
      "Epoch: 0, Batch: 1755, Loss: nan\n",
      "Epoch: 0, Batch: 1756, Loss: nan\n",
      "Epoch: 0, Batch: 1757, Loss: nan\n",
      "Epoch: 0, Batch: 1758, Loss: nan\n",
      "Epoch: 0, Batch: 1759, Loss: nan\n",
      "Epoch: 0, Batch: 1760, Loss: nan\n",
      "Epoch: 0, Batch: 1761, Loss: nan\n",
      "Epoch: 0, Batch: 1762, Loss: nan\n",
      "Epoch: 0, Batch: 1763, Loss: nan\n",
      "Epoch: 0, Batch: 1764, Loss: nan\n",
      "Epoch: 0, Batch: 1765, Loss: nan\n",
      "Epoch: 0, Batch: 1766, Loss: nan\n",
      "Epoch: 0, Batch: 1767, Loss: nan\n",
      "Epoch: 0, Batch: 1768, Loss: nan\n",
      "Epoch: 0, Batch: 1769, Loss: nan\n",
      "Epoch: 0, Batch: 1770, Loss: nan\n",
      "Epoch: 0, Batch: 1771, Loss: nan\n",
      "Epoch: 0, Batch: 1772, Loss: nan\n",
      "Epoch: 0, Batch: 1773, Loss: nan\n",
      "Epoch: 0, Batch: 1774, Loss: nan\n",
      "Epoch: 0, Batch: 1775, Loss: nan\n",
      "Epoch: 0, Batch: 1776, Loss: nan\n",
      "Epoch: 0, Batch: 1777, Loss: nan\n",
      "Epoch: 0, Batch: 1778, Loss: nan\n",
      "Epoch: 0, Batch: 1779, Loss: nan\n",
      "Epoch: 0, Batch: 1780, Loss: nan\n",
      "Epoch: 0, Batch: 1781, Loss: nan\n",
      "Epoch: 0, Batch: 1782, Loss: nan\n",
      "Epoch: 0, Batch: 1783, Loss: nan\n",
      "Epoch: 0, Batch: 1784, Loss: nan\n",
      "Epoch: 0, Batch: 1785, Loss: nan\n",
      "Epoch: 0, Batch: 1786, Loss: nan\n",
      "Epoch: 0, Batch: 1787, Loss: nan\n",
      "Epoch: 0, Batch: 1788, Loss: nan\n",
      "Epoch: 0, Batch: 1789, Loss: nan\n",
      "Epoch: 0, Batch: 1790, Loss: nan\n",
      "Epoch: 0, Batch: 1791, Loss: nan\n",
      "Epoch: 0, Batch: 1792, Loss: nan\n",
      "Epoch: 0, Batch: 1793, Loss: nan\n",
      "Epoch: 0, Batch: 1794, Loss: nan\n",
      "Epoch: 0, Batch: 1795, Loss: nan\n",
      "Epoch: 0, Batch: 1796, Loss: nan\n",
      "Epoch: 0, Batch: 1797, Loss: nan\n",
      "Epoch: 0, Batch: 1798, Loss: nan\n",
      "Epoch: 0, Batch: 1799, Loss: nan\n",
      "Epoch: 0, Batch: 1800, Loss: nan\n",
      "Epoch: 0, Batch: 1801, Loss: nan\n",
      "Epoch: 0, Batch: 1802, Loss: nan\n",
      "Epoch: 0, Batch: 1803, Loss: nan\n",
      "Epoch: 0, Batch: 1804, Loss: nan\n",
      "Epoch: 0, Batch: 1805, Loss: nan\n",
      "Epoch: 0, Batch: 1806, Loss: nan\n",
      "Epoch: 0, Batch: 1807, Loss: nan\n",
      "Epoch: 0, Batch: 1808, Loss: nan\n",
      "Epoch: 0, Batch: 1809, Loss: nan\n",
      "Epoch: 0, Batch: 1810, Loss: nan\n",
      "Epoch: 0, Batch: 1811, Loss: nan\n",
      "Epoch: 0, Batch: 1812, Loss: nan\n",
      "Epoch: 0, Batch: 1813, Loss: nan\n",
      "Epoch: 0, Batch: 1814, Loss: nan\n",
      "Epoch: 0, Batch: 1815, Loss: nan\n",
      "Epoch: 0, Batch: 1816, Loss: nan\n",
      "Epoch: 0, Batch: 1817, Loss: nan\n",
      "Epoch: 0, Batch: 1818, Loss: nan\n",
      "Epoch: 0, Batch: 1819, Loss: nan\n",
      "Epoch: 0, Batch: 1820, Loss: nan\n",
      "Epoch: 0, Batch: 1821, Loss: nan\n",
      "Epoch: 0, Batch: 1822, Loss: nan\n",
      "Epoch: 0, Batch: 1823, Loss: nan\n",
      "Epoch: 0, Batch: 1824, Loss: nan\n",
      "Epoch: 0, Batch: 1825, Loss: nan\n",
      "Epoch: 0, Batch: 1826, Loss: nan\n",
      "Epoch: 0, Batch: 1827, Loss: nan\n",
      "Epoch: 0, Batch: 1828, Loss: nan\n",
      "Epoch: 0, Batch: 1829, Loss: nan\n",
      "Epoch: 0, Batch: 1830, Loss: nan\n",
      "Epoch: 0, Batch: 1831, Loss: nan\n",
      "Epoch: 0, Batch: 1832, Loss: nan\n",
      "Epoch: 0, Batch: 1833, Loss: nan\n",
      "Epoch: 0, Batch: 1834, Loss: nan\n",
      "Epoch: 0, Batch: 1835, Loss: nan\n",
      "Epoch: 0, Batch: 1836, Loss: nan\n",
      "Epoch: 0, Batch: 1837, Loss: nan\n",
      "Epoch: 0, Batch: 1838, Loss: nan\n",
      "Epoch: 0, Batch: 1839, Loss: nan\n",
      "Epoch: 0, Batch: 1840, Loss: nan\n",
      "Epoch: 0, Batch: 1841, Loss: nan\n",
      "Epoch: 0, Batch: 1842, Loss: nan\n",
      "Epoch: 0, Batch: 1843, Loss: nan\n",
      "Epoch: 0, Batch: 1844, Loss: nan\n",
      "Epoch: 0, Batch: 1845, Loss: nan\n",
      "Epoch: 0, Batch: 1846, Loss: nan\n",
      "Epoch: 0, Batch: 1847, Loss: nan\n",
      "Epoch: 0, Batch: 1848, Loss: nan\n",
      "Epoch: 0, Batch: 1849, Loss: nan\n",
      "Epoch: 0, Batch: 1850, Loss: nan\n",
      "Epoch: 0, Batch: 1851, Loss: nan\n",
      "Epoch: 0, Batch: 1852, Loss: nan\n",
      "Epoch: 0, Batch: 1853, Loss: nan\n",
      "Epoch: 0, Batch: 1854, Loss: nan\n",
      "Epoch: 0, Batch: 1855, Loss: nan\n",
      "Epoch: 0, Batch: 1856, Loss: nan\n",
      "Epoch: 0, Batch: 1857, Loss: nan\n",
      "Epoch: 0, Batch: 1858, Loss: nan\n",
      "Epoch: 0, Batch: 1859, Loss: nan\n",
      "Epoch: 0, Batch: 1860, Loss: nan\n",
      "Epoch: 0, Batch: 1861, Loss: nan\n",
      "Epoch: 0, Batch: 1862, Loss: nan\n",
      "Epoch: 0, Batch: 1863, Loss: nan\n",
      "Epoch: 0, Batch: 1864, Loss: nan\n",
      "Epoch: 0, Batch: 1865, Loss: nan\n",
      "Epoch: 0, Batch: 1866, Loss: nan\n",
      "Epoch: 0, Batch: 1867, Loss: nan\n",
      "Epoch: 0, Batch: 1868, Loss: nan\n",
      "Epoch: 0, Batch: 1869, Loss: nan\n",
      "Epoch: 0, Batch: 1870, Loss: nan\n",
      "Epoch: 0, Batch: 1871, Loss: nan\n",
      "Epoch: 0, Batch: 1872, Loss: nan\n",
      "Epoch: 0, Batch: 1873, Loss: nan\n",
      "Epoch: 0, Batch: 1874, Loss: nan\n",
      "Epoch: 0, Batch: 1875, Loss: nan\n",
      "Epoch: 0, Batch: 1876, Loss: nan\n",
      "Epoch: 0, Batch: 1877, Loss: nan\n",
      "Epoch: 0, Batch: 1878, Loss: nan\n",
      "Epoch: 0, Batch: 1879, Loss: nan\n",
      "Epoch: 0, Batch: 1880, Loss: nan\n",
      "Epoch: 0, Batch: 1881, Loss: nan\n",
      "Epoch: 0, Batch: 1882, Loss: nan\n",
      "Epoch: 0, Batch: 1883, Loss: nan\n",
      "Epoch: 0, Batch: 1884, Loss: nan\n",
      "Epoch: 0, Batch: 1885, Loss: nan\n",
      "Epoch: 0, Batch: 1886, Loss: nan\n",
      "Epoch: 0, Batch: 1887, Loss: nan\n",
      "Epoch: 0, Batch: 1888, Loss: nan\n",
      "Epoch: 0, Batch: 1889, Loss: nan\n",
      "Epoch: 0, Batch: 1890, Loss: nan\n",
      "Epoch: 0, Batch: 1891, Loss: nan\n",
      "Epoch: 0, Batch: 1892, Loss: nan\n",
      "Epoch: 0, Batch: 1893, Loss: nan\n",
      "Epoch: 0, Batch: 1894, Loss: nan\n",
      "Epoch: 0, Batch: 1895, Loss: nan\n",
      "Epoch: 0, Batch: 1896, Loss: nan\n",
      "Epoch: 0, Batch: 1897, Loss: nan\n",
      "Epoch: 0, Batch: 1898, Loss: nan\n",
      "Epoch: 0, Batch: 1899, Loss: nan\n",
      "Epoch: 0, Batch: 1900, Loss: nan\n",
      "Epoch: 0, Batch: 1901, Loss: nan\n",
      "Epoch: 0, Batch: 1902, Loss: nan\n",
      "Epoch: 0, Batch: 1903, Loss: nan\n",
      "Epoch: 0, Batch: 1904, Loss: nan\n",
      "Epoch: 0, Batch: 1905, Loss: nan\n",
      "Epoch: 0, Batch: 1906, Loss: nan\n",
      "Epoch: 0, Batch: 1907, Loss: nan\n",
      "Epoch: 0, Batch: 1908, Loss: nan\n",
      "Epoch: 0, Batch: 1909, Loss: nan\n",
      "Epoch: 0, Batch: 1910, Loss: nan\n",
      "Epoch: 0, Batch: 1911, Loss: nan\n",
      "Epoch: 0, Batch: 1912, Loss: nan\n",
      "Epoch: 0, Batch: 1913, Loss: nan\n",
      "Epoch: 0, Batch: 1914, Loss: nan\n",
      "Epoch: 0, Batch: 1915, Loss: nan\n",
      "Epoch: 0, Batch: 1916, Loss: nan\n",
      "Epoch: 0, Batch: 1917, Loss: nan\n",
      "Epoch: 0, Batch: 1918, Loss: nan\n",
      "Epoch: 0, Batch: 1919, Loss: nan\n",
      "Epoch: 0, Batch: 1920, Loss: nan\n",
      "Epoch: 0, Batch: 1921, Loss: nan\n",
      "Epoch: 0, Batch: 1922, Loss: nan\n",
      "Epoch: 0, Batch: 1923, Loss: nan\n",
      "Epoch: 0, Batch: 1924, Loss: nan\n",
      "Epoch: 0, Batch: 1925, Loss: nan\n",
      "Epoch: 0, Batch: 1926, Loss: nan\n",
      "Epoch: 0, Batch: 1927, Loss: nan\n",
      "Epoch: 0, Batch: 1928, Loss: nan\n",
      "Epoch: 0, Batch: 1929, Loss: nan\n",
      "Epoch: 0, Batch: 1930, Loss: nan\n",
      "Epoch: 0, Batch: 1931, Loss: nan\n",
      "Epoch: 0, Batch: 1932, Loss: nan\n",
      "Epoch: 0, Batch: 1933, Loss: nan\n",
      "Epoch: 0, Batch: 1934, Loss: nan\n",
      "Epoch: 0, Batch: 1935, Loss: nan\n",
      "Epoch: 0, Batch: 1936, Loss: nan\n",
      "Epoch: 0, Batch: 1937, Loss: nan\n",
      "Epoch: 0, Batch: 1938, Loss: nan\n",
      "Epoch: 0, Batch: 1939, Loss: nan\n",
      "Epoch: 0, Batch: 1940, Loss: nan\n",
      "Epoch: 0, Batch: 1941, Loss: nan\n",
      "Epoch: 0, Batch: 1942, Loss: nan\n",
      "Epoch: 0, Batch: 1943, Loss: nan\n",
      "Epoch: 0, Batch: 1944, Loss: nan\n",
      "Epoch: 0, Batch: 1945, Loss: nan\n",
      "Epoch: 0, Batch: 1946, Loss: nan\n",
      "Epoch: 0, Batch: 1947, Loss: nan\n",
      "Epoch: 0, Batch: 1948, Loss: nan\n",
      "Epoch: 0, Batch: 1949, Loss: nan\n",
      "Epoch: 0, Batch: 1950, Loss: nan\n",
      "Epoch: 0, Batch: 1951, Loss: nan\n",
      "Epoch: 0, Batch: 1952, Loss: nan\n",
      "Epoch: 0, Batch: 1953, Loss: nan\n",
      "Epoch: 0, Batch: 1954, Loss: nan\n",
      "Epoch: 0, Batch: 1955, Loss: nan\n",
      "Epoch: 0, Batch: 1956, Loss: nan\n",
      "Epoch: 0, Batch: 1957, Loss: nan\n",
      "Epoch: 0, Batch: 1958, Loss: nan\n",
      "Epoch: 0, Batch: 1959, Loss: nan\n",
      "Epoch: 0, Batch: 1960, Loss: nan\n",
      "Epoch: 0, Batch: 1961, Loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m out, indices \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[0;32m     30\u001b[0m rec_loss \u001b[38;5;241m=\u001b[39m loss(out, x)\n\u001b[1;32m---> 31\u001b[0m \u001b[43mrec_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Batch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi_batch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrec_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\John\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\John\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = 3\n",
    "seed = 44\n",
    "EPOCHS = 25\n",
    "BATCH_SIZE = 64\n",
    "D = train_input_array.shape[1]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "torch.random.manual_seed(seed)\n",
    "model = VQVAE(input_dim=128, codebook_size=512, codebook_dim=64)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "log_loss = np.array([])\n",
    "for i_epoch in range(EPOCHS):\n",
    "    dataloader = DataLoader(train_input_array, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    for i_batch, batch in enumerate(dataloader):\n",
    "        x = batch.to(device)\n",
    "        x = x.to(torch.float)\n",
    "        x = x.reshape(BATCH_SIZE, 1, D)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out, indices = model(x)\n",
    "\n",
    "        rec_loss = loss(out, x)\n",
    "        rec_loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch: {i_epoch}, Batch: {i_batch}, Loss: {rec_loss}\")\n",
    "        log_loss = np.append(log_loss,np.log(rec_loss.detach().cpu().numpy()))\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': i_epoch,\n",
    "            'model_state_dict':  model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'log_loss': log_loss,\n",
    "        }, f\"vqvae_vimeo_checkpoint.pth\")\n",
    "\n",
    "plt.plot(log_loss)\n",
    "plt.savefig(\"vqvae_vimeo_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m     20\u001b[0m inputs \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 21\u001b[0m recon_batch, indices \u001b[38;5;241m=\u001b[39m \u001b[43mvqvae\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m     24\u001b[0m recon_loss \u001b[38;5;241m=\u001b[39m criterion(recon_batch, inputs)\n",
      "File \u001b[1;32mc:\\Users\\John\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\John\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 50\u001b[0m, in \u001b[0;36mVQVAE.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     49\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode(x)\n\u001b[1;32m---> 50\u001b[0m     quantized, indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     x_recon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(quantized)\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x_recon, indices\n",
      "Cell \u001b[1;32mIn[10], line 39\u001b[0m, in \u001b[0;36mVQVAE.quantize\u001b[1;34m(self, z)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquantize\u001b[39m(\u001b[38;5;28mself\u001b[39m, z):\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m# Find nearest embedding in the codebook\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     z \u001b[38;5;241m=\u001b[39m z\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 39\u001b[0m     distances \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(\u001b[43mz\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodebook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     40\u001b[0m     indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmin(distances, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     41\u001b[0m     quantized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcodebook(indices)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(vqvae.parameters(), lr=0.001)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "vqvae.to(device)\n",
    "\n",
    "# Training loop\n",
    "epochs = 25\n",
    "for epoch in range(epochs):\n",
    "    vqvae.train()\n",
    "    total_recon_loss = 0.0\n",
    "    total_commitment_loss = 0.0\n",
    "    \n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        inputs = data.to(device)\n",
    "        recon_batch, indices = vqvae(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        recon_loss = criterion(recon_batch, inputs)\n",
    "        commitment_loss = torch.mean(torch.norm((indices.float() - vqvae.codebook(indices)) ** 2))\n",
    "        loss = recon_loss + commitment_loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss\n",
    "        total_recon_loss += recon_loss.item()\n",
    "        total_commitment_loss += commitment_loss.item()\n",
    "        \n",
    "    # Print epoch statistics\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Recon Loss: {total_recon_loss / len(train_loader):.4f}, Commitment Loss: {total_commitment_loss / len(train_loader):.4f}\")\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learned Compression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tensorflow as tf\n",
    "\n",
    "from compressai.models import CompressionModel\n",
    "from compressai.models.utils import conv, deconv\n",
    "from compressai.entropy_models import EntropyBottleneck\n",
    "from compressor_compressai import Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelToSpatial(nn.Module):\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, spatial_dim = x.size()\n",
    "        return x.view(batch_size, spatial_dim, channels)\n",
    "\n",
    "\n",
    "class Network(CompressionModel):\n",
    "    def __init__(self, compressed_d = 64, uncompressed_d = 128):\n",
    "        \"\"\"\n",
    "        compressed_d = compressed dimension of the embedding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.entropy_bottleneck = EntropyBottleneck(1)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, compressed_d, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(compressed_d, compressed_d, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(compressed_d, compressed_d, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(compressed_d, compressed_d//2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(compressed_d//2, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyLinear(compressed_d),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyLinear(compressed_d)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(32, compressed_d, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(compressed_d, uncompressed_d, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(uncompressed_d, uncompressed_d, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(uncompressed_d, uncompressed_d, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(uncompressed_d, uncompressed_d//2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(uncompressed_d//2, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyLinear(uncompressed_d),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyLinear(uncompressed_d)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.encoder(x)\n",
    "        \n",
    "        y_hat, y_likelihoods = self.entropy_bottleneck(y)\n",
    "\n",
    "        x_hat = self.decoder(y_hat)\n",
    "\n",
    "        return x_hat, y_likelihoods\n",
    "    \n",
    "    def get_compressed_embeddings(self, x):\n",
    "        y = self.encoder(x)\n",
    "        \n",
    "        y_hat, y_likelihoods = self.entropy_bottleneck(y)\n",
    "\n",
    "        return y_hat, y_likelihoods\n",
    "    \n",
    "    def decode_compressed_embeddings(self, y_hat):\n",
    "        \n",
    "        x_hat = self.decoder(y_hat)\n",
    "\n",
    "        return x_hat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "# hyperparamters    \n",
    "seed = 44\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 64\n",
    "lmbda = 1 # parameter for bitrate distortion tradeoff\n",
    "\n",
    "torch.random.manual_seed(seed)\n",
    "model = Network(64).to(\"cuda\")\n",
    "model.train()\n",
    "\n",
    "# parameters\n",
    "parameters = set(p for n, p in model.named_parameters() if not n.endswith(\".quantiles\"))\n",
    "aux_parameters = set(p for n, p in model.named_parameters() if n.endswith(\".quantiles\"))\n",
    "optimizer = optim.Adam(parameters, lr=1e-4)\n",
    "aux_optimizer = optim.Adam(aux_parameters, lr=1e-3)\n",
    "\n",
    "D = train_input_array.shape[1]\n",
    "\n",
    "loss_arr = np.array([])\n",
    "aux_loss_arr = np.array([])\n",
    "mse_loss_arr = np.array([])\n",
    "for i_epoch in range(EPOCHS):\n",
    "    dataloader = DataLoader(train_input_array, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    for i_batch, batch in enumerate(dataloader):\n",
    "        x = batch.to(\"cuda\")\n",
    "        x = x.to(torch.float)\n",
    "        x = x.reshape(BATCH_SIZE, 1, D)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        aux_optimizer.zero_grad()\n",
    "\n",
    "        x_hat, y_likelihoods = model(x)\n",
    "\n",
    "        # bitrate of the quantized latent\n",
    "        N, _, L = x.size()\n",
    "        num_logits = N * L\n",
    "        bpp_loss = torch.log(y_likelihoods).sum() / (-math.log(2) * num_logits)\n",
    "\n",
    "        # mean square error\n",
    "        mse_loss = F.mse_loss(x, x_hat)\n",
    "\n",
    "        # final loss term\n",
    "        loss = mse_loss + lmbda * bpp_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        aux_loss = model.aux_loss()\n",
    "        aux_optimizer.step()\n",
    "        print(f\"Epoch: {i_epoch}, Batch: {i_batch}, loss: {loss}, aux_loss: {aux_loss}\")\n",
    "        loss_arr = np.append(loss_arr,loss.detach().cpu().numpy())\n",
    "        aux_loss_arr = np.append(aux_loss_arr,aux_loss.detach().cpu().numpy())\n",
    "        mse_loss_arr = np.append(mse_loss_arr,mse_loss.detach().cpu().numpy())\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': i_epoch,\n",
    "            'model_state_dict':  model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss_arr': loss_arr,\n",
    "            'aux_loss_arr': aux_loss_arr,\n",
    "            'mse_loss_arr': mse_loss_arr\n",
    "        }, f\"compressai_losses.pth\")\n",
    "\n",
    "plt.plot(loss_arr)\n",
    "plt.plot(aux_loss_arr)\n",
    "plt.savefig(\"compressai_losses.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\John\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 52.  24. -64. ...  58.  39. -52.]\n",
      " [ 79.  30. -45. ...  72.  40. -39.]\n",
      " [ 52.  33. -25. ...  52.  26. -66.]\n",
      " ...\n",
      " [ 76.  36. -82. ...  31.  41. -65.]\n",
      " [ 37.  17. -42. ...  44.  43. -54.]\n",
      " [ 80.  32. -29. ...  57.  30. -69.]]\n",
      "145.0\n",
      "-153.0\n"
     ]
    }
   ],
   "source": [
    "# Compressed Embeddings Playground\n",
    "\n",
    "model = Network(64)\n",
    "\n",
    "checkpoint = torch.load(\"compressai_losses_new.pth\")\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "model.to(\"cuda\")\n",
    "\n",
    "# for stacking tensors back into numpy array\n",
    "tensor_list = []\n",
    "\n",
    "test_dataloader = DataLoader(test_input_array, batch_size=1, shuffle=False)\n",
    "for i_batch, batch in enumerate(test_dataloader):\n",
    "        x = batch.to(\"cuda\")\n",
    "        x = x.to(torch.float)\n",
    "        x = x.reshape(1, 1, 128)\n",
    "\n",
    "        #print(x)\n",
    "        #x_hat, y_likelihoods = model(x)\n",
    "        #print(x_hat.to(torch.int))\n",
    "\n",
    "        y_hat, y_likelihoods = model.get_compressed_embeddings(x)\n",
    "\n",
    "        y_hat = y_hat.reshape(64)\n",
    "        tensor_list.append(y_hat)\n",
    "\n",
    "        if i_batch > 100:\n",
    "                break\n",
    "\n",
    "\n",
    "combined_tensor = torch.stack(tensor_list)\n",
    "compressed_embeddings = combined_tensor.detach().cpu().numpy()\n",
    "print(compressed_embeddings)\n",
    "\n",
    "\n",
    "print(max(compressed_embeddings.flatten()))\n",
    "print(min(compressed_embeddings.flatten()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building compressed train split\n",
      "Progress: 0.0%\n",
      "Progress: 10.0%\n",
      "Progress: 20.0%\n",
      "Progress: 30.0%\n",
      "Progress: 40.0%\n",
      "Progress: 50.0%\n",
      "Progress: 60.0%\n",
      "Progress: 70.0%\n",
      "Progress: 80.0%\n",
      "Progress: 90.0%\n",
      "Progress: 100.0%\n",
      "Stacking tensors and converting to numpy array\n"
     ]
    }
   ],
   "source": [
    "# Run train set back through model to get compressed embeddings train split\n",
    "\n",
    "compress_batch_size = 64\n",
    "D = train_input_array.shape[1]\n",
    "\n",
    "model = Network(64)\n",
    "\n",
    "checkpoint = torch.load(\"compressai_losses.pth\",map_location=\"cuda\")\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "model.to(\"cuda\")\n",
    "\n",
    "print(\"Building compressed train split\")\n",
    "\n",
    "# for stacking tensors back into numpy array\n",
    "tensor_list = []\n",
    "\n",
    "compress_dataloader = DataLoader(train_input_array, batch_size=compress_batch_size, shuffle=False)\n",
    "for i_batch, batch in enumerate(compress_dataloader):\n",
    "        x = batch.to(\"cuda\")\n",
    "        x = x.to(torch.float)\n",
    "        x = x.reshape(compress_batch_size, 1, D)\n",
    "\n",
    "        y_hat, y_likelihoods = model.get_compressed_embeddings(x)\n",
    "\n",
    "        y_hat = y_hat.to(torch.int)\n",
    "\n",
    "        for i in range(y_hat.size(0)): \n",
    "                train_entry = y_hat[i, 0, :]  \n",
    "                tensor_list.append(train_entry)  \n",
    "\n",
    "        if i_batch % 1562 == 0:\n",
    "                print(\"Progress: \" + str((i_batch/1562)*10) + \"%\")\n",
    "\n",
    "print(\"Stacking tensors and converting to numpy array\")\n",
    "\n",
    "combined_tensor = torch.stack(tensor_list)\n",
    "compressed_train_embeddings = combined_tensor.detach().cpu().numpy()\n",
    "\n",
    "compressed_train_split_path = 'learned_compressed_train_split.npy'\n",
    "\n",
    "np.save(compressed_train_split_path, compressed_train_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building compressed test split\n",
      "Stacking tensors and converting to numpy array\n"
     ]
    }
   ],
   "source": [
    "# Run test set through model to get compressed embeddings test split\n",
    "\n",
    "compress_batch_size = 64\n",
    "D = test_input_array.shape[1]\n",
    "\n",
    "model = Network(64)\n",
    "\n",
    "checkpoint = torch.load(\"compressai_losses.pth\",map_location=\"cuda\")\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "model.to(\"cuda\")\n",
    "\n",
    "print(\"Building compressed test split\")\n",
    "\n",
    "# for stacking tensors back into numpy array\n",
    "tensor_list = []\n",
    "\n",
    "compress_dataloader = DataLoader(test_input_array, batch_size=compress_batch_size, shuffle=False)\n",
    "for i_batch, batch in enumerate(compress_dataloader):\n",
    "        x = batch.to(\"cuda\")\n",
    "        x = x.to(torch.float)\n",
    "\n",
    "        if i_batch == 156:\n",
    "            x = x.reshape(16, 1, D)\n",
    "        else:\n",
    "            x = x.reshape(compress_batch_size, 1, D)\n",
    "                \n",
    "         \n",
    "\n",
    "        y_hat, y_likelihoods = model.get_compressed_embeddings(x)\n",
    "\n",
    "        y_hat = y_hat.to(torch.int) # 16 bit int\n",
    "\n",
    "        for i in range(y_hat.size(0)): \n",
    "            test_entry = y_hat[i, 0, :]  \n",
    "            tensor_list.append(test_entry) \n",
    "\n",
    "        #if i_batch % 1562 == 0:\n",
    "            #print(\"Progress: \" + str((i_batch/1562)*10) + \"%\")\n",
    "\n",
    "print(\"Stacking tensors and converting to numpy array\")\n",
    "\n",
    "combined_tensor = torch.stack(tensor_list)\n",
    "compressed_test_embeddings = combined_tensor.detach().cpu().numpy()\n",
    "\n",
    "compressed_test_split_path = 'learned_compressed_test_split.npy'\n",
    "\n",
    "np.save(compressed_test_split_path, compressed_test_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load compressed embedding splits\n",
    "\n",
    "compressed_train_split_path = 'learned_compressed_train_split.npy'\n",
    "compressed_test_split_path = 'learned_compressed_test_split.npy'\n",
    "\n",
    "compressed_train_split_for_hnsw = np.load(compressed_train_split_path)\n",
    "\n",
    "#print(compressed_train_split_for_hnsw)\n",
    "\n",
    "#print(compressed_train_split_for_hnsw[0])\n",
    "\n",
    "#compressed_train_split_for_hnsw.shape[0]\n",
    "\n",
    "compressed_test_split_for_hnsw = np.load(compressed_test_split_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "import random, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = compressed_train_split_for_hnsw.shape[1]\n",
    "\n",
    "hnsw_indexer = faiss.index_factory(dim, \"HNSW\")\n",
    "dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hnsw_indexer.train(compressed_train_split_for_hnsw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hnsw_indexer.add(compressed_train_split_for_hnsw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6092., 6689., 6719., 6729., 6759., 6867., 6910., 6964., 7038.,\n",
       "        7184.],\n",
       "       [5453., 5913., 6064., 6158., 6316., 6328., 6408., 6420., 6461.,\n",
       "        6474.]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 10\n",
    "distances, index = hnsw_indexer.search(compressed_test_split_for_hnsw[:2], k)\n",
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "528129506"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Size\n",
    "\n",
    "idx_file = open(\"./temp.index\", \"w\")\n",
    "idx_file.truncate(0)\n",
    "idx_file.close()\n",
    "faiss.write_index(hnsw_indexer, \"./temp.index\")\n",
    "file_size = os.path.getsize('./temp.index')\n",
    "file_size\n",
    "#528129506 528mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7074199999999999\n"
     ]
    }
   ],
   "source": [
    "# Relevancy - what percent of the K neighbor search results are in the top T correct closest neighbors\n",
    "\n",
    "k = 5 # number of returned neighbors for each embedding\n",
    "t = 25 # top X number of closest neighbors\n",
    "\n",
    "distances, index = hnsw_indexer.search(compressed_test_split_for_hnsw[:10000], k)\n",
    "\n",
    "relevantPercent = []\n",
    "\n",
    "i = 0\n",
    "for search_results in index:\n",
    "    closest_neighbors = neighbors[i]\n",
    "    \n",
    "    topIndexs = []\n",
    "    for x in range(t):\n",
    "        index_dict = closest_neighbors[x]\n",
    "        topIndexs.append(index_dict[\"index\"])\n",
    "\n",
    "    count = 0\n",
    "    for x in search_results:\n",
    "        if x in topIndexs:\n",
    "            count+=1\n",
    "\n",
    "    relevantPercent.append(count/k)\n",
    "    i=i+1\n",
    "    \n",
    "print(sum(relevantPercent) / len(relevantPercent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg search time:  0.0003019919395446777  seconds\n"
     ]
    }
   ],
   "source": [
    "# Search Speed\n",
    "\n",
    "total_time = 0\n",
    "iterations = 1000\n",
    "\n",
    "for x in range(iterations):\n",
    "\n",
    "    k = 25\n",
    "    index = random.randint(0, 9999)\n",
    "\n",
    "    input = np.empty((0,64))\n",
    "    input = np.vstack([input,compressed_test_split_for_hnsw[index]])\n",
    "\n",
    "    start_time = time.time()\n",
    "    distances, index = hnsw_indexer.search(input, k)\n",
    "    end_time = time.time()\n",
    "\n",
    "    total_time += end_time - start_time\n",
    "\n",
    "\n",
    "print(\"avg search time: \", total_time/iterations, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss\n",
      "94.27203709304712\n"
     ]
    }
   ],
   "source": [
    "# Test Reconstruction Loss\n",
    "\n",
    "compress_batch_size = 64\n",
    "D = test_input_array.shape[1]\n",
    "\n",
    "model = Network(64)\n",
    "\n",
    "checkpoint = torch.load(\"compressai_losses.pth\",map_location=\"cuda\")\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "model.to(\"cuda\")\n",
    "\n",
    "print(\"Test Reconstruction Loss:\")\n",
    "\n",
    "loss_arr = np.array([])\n",
    "mse_loss_arr = np.array([])\n",
    "\n",
    "lmbda = 1\n",
    "\n",
    "compress_dataloader = DataLoader(test_input_array, batch_size=compress_batch_size, shuffle=False)\n",
    "for i_batch, batch in enumerate(compress_dataloader):\n",
    "        x = batch.to(\"cuda\")\n",
    "        x = x.to(torch.float)\n",
    "\n",
    "        if i_batch == 156:\n",
    "            x = x.reshape(16, 1, D)\n",
    "        else:\n",
    "            x = x.reshape(compress_batch_size, 1, D)\n",
    "                \n",
    "        x_hat, y_likelihoods = model(x)\n",
    "\n",
    "        # bitrate of the quantized latent\n",
    "        N, _, L = x.size()\n",
    "        num_logits = N * L\n",
    "        bpp_loss = torch.log(y_likelihoods).sum() / (-math.log(2) * num_logits)\n",
    "\n",
    "        # mean square error\n",
    "        mse_loss = F.mse_loss(x, x_hat)\n",
    "\n",
    "        # final loss term\n",
    "        loss = mse_loss + lmbda * bpp_loss\n",
    "\n",
    "        loss_arr = np.append(loss_arr,loss.detach().cpu().numpy())\n",
    "        mse_loss_arr = np.append(mse_loss_arr,mse_loss.detach().cpu().numpy())\n",
    "\n",
    "print(np.average(loss_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\John\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encode and Decode Time:\n",
      "Avg encode time: 0.13771460466324145\n",
      "Avg decode time: 0.05842156167242937\n"
     ]
    }
   ],
   "source": [
    "# Decode and Encode speed\n",
    "\n",
    "compress_batch_size = 64\n",
    "D = test_input_array.shape[1]\n",
    "\n",
    "model = Network(64)\n",
    "\n",
    "checkpoint = torch.load(\"compressai_losses.pth\",map_location=\"cuda\")\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "model.to(\"cuda\")\n",
    "\n",
    "print(\"Encode and Decode Time:\")\n",
    "\n",
    "total_encode_time_array = np.array([])\n",
    "total_decode_time_array = np.array([])\n",
    "total_encode_time = 0\n",
    "total_decode_time = 0\n",
    "\n",
    "compress_dataloader = DataLoader(test_input_array, batch_size=compress_batch_size, shuffle=False)\n",
    "for i_batch, batch in enumerate(compress_dataloader):\n",
    "        x = batch.to(\"cuda\")\n",
    "        x = x.to(torch.float)\n",
    "\n",
    "        if i_batch == 156:\n",
    "            x = x.reshape(16, 1, D)\n",
    "        else:\n",
    "            x = x.reshape(compress_batch_size, 1, D)\n",
    "\n",
    "        start_time = time.time()\n",
    "        y_hat, y_likelihoods = model.get_compressed_embeddings(x)\n",
    "        end_time = time.time()\n",
    "        total_encode_time += end_time - start_time\n",
    "\n",
    "        start_time = time.time()\n",
    "        x_hat = model.decode_compressed_embeddings(y_hat)\n",
    "        end_time = time.time()\n",
    "        total_decode_time += end_time - start_time\n",
    "\n",
    "        total_encode_time_array = np.append(total_encode_time_array,total_encode_time)\n",
    "        total_decode_time_array = np.append(total_decode_time_array,total_decode_time)\n",
    "\n",
    "print(\"Avg encode time: \" + str(np.average(total_encode_time_array)))\n",
    "print(\"Avg decode time: \" + str(np.average(total_decode_time_array)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
